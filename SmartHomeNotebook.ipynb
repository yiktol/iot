{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Smart Home Sensor Analysis</h1>\n",
    "<p>The ‘Household Power Consumption‘ dataset is a multivariate time series dataset that describes the electricity consumption for a single household for last few months. The  dataset is modeled after household consumption dataset available here - \n",
    "https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption\n",
    "\n",
    "\n",
    "It is a multivariate series comprised of seven variables (besides the date and time); they are:\n",
    "\n",
    "<b>global_active_power:</b> The total active power consumed by the household (kilowatts).<br>\n",
    "<b>global_reactive_power:</b> The total reactive power consumed by the household (kilowatts).<br>\n",
    "<b>voltage:</b> Average voltage (volts).<br>\n",
    "<b>global_intensity:</b> Average current intensity (amps).<br>\n",
    "<b>sub_metering_1:</b> Active energy for kitchen (watt-hours of active energy).<br>\n",
    "<b>sub_metering_2:</b> Active energy for laundry (watt-hours of active energy).<br>\n",
    "<b>sub_metering_3:</b> Active energy for climate control systems (watt-hours of active energy).<br>\n",
    "\n",
    "<p> In the following section, we will analyze and predict hourly power consumption using DeepAR on SageMaker. The purpose of this exercise is to demonstrate Sagemaker integration with IoT Analytics and not to focus on training the right model to generate accurate prediction.\n",
    "<p>For more information see the DeepAR [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) or [paper](https://arxiv.org/abs/1704.04110), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox\n",
    "\n",
    "from sagemaker import get_execution_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we can override the default values for the following:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()# replace with an existing bucket if needed\n",
    "\n",
    "s3_prefix = 'iot-analytics-demo-notebook'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import  dataset from the IotAnalytics database and upload it to S3 to make it available for Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading from the IotAnalytics database\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy import isnan\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy import isnan\n",
    "\n",
    "\n",
    "bucket='iotareinvent18'\n",
    "data_key = 'inputdata.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "\n",
    "# Function to convert date columns into a single timestamp field\n",
    "#def parse(x):\n",
    "#    return datetime.strptime(x, '%Y %m %d %H')\n",
    "def parse(x):\n",
    "    t= pd.to_datetime(str(x)) \n",
    "    timestring = t.strftime('%Y.%m.%d %H:%M:%S')\n",
    "    return t\n",
    "\n",
    "def fill_missing(values):\n",
    "    one_day = 60 * 24\n",
    "    for row in range(values.shape[0]):\n",
    "        for col in range(values.shape[1]):\n",
    "            if np.isnan(values[row, col]):\n",
    "                values[row, col] = values[row - one_day, col]\n",
    " \n",
    "# load sample data\n",
    "\n",
    "#n = sum(1 for line in open(data_location)) - 1 #number of records in file (excludes header)\n",
    "n=244207 #total observations\n",
    "s = 10000 #desired sample size\n",
    "sampling = sorted(random.sample(range(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list\n",
    "\n",
    "dataset = pd.read_csv(data_location,  header=0, skiprows=sampling ,low_memory=False, infer_datetime_format=True, date_parser = parse)\n",
    "dataset['__dt'] = dataset['timestamp']\n",
    "dataset['cost'] = (dataset['sub_metering_1'] +dataset['sub_metering_2'] + dataset['sub_metering_3'])*1.5\n",
    "dataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\n",
    "dataset.set_index('timestamp', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Examine the dataset\n",
      "dataset shape  (10000, 9)\n",
      "                     global_active_power  global_reactive_power  voltage  \\\n",
      "timestamp                                                                  \n",
      "2018-09-07 10:34:00                 0.16                   0.06   241.26   \n",
      "2018-07-06 04:17:00                 0.92                   0.27   240.77   \n",
      "2018-09-10 10:05:00                 0.38                   0.17   241.58   \n",
      "2018-08-04 15:15:00                 0.31                   0.22   242.08   \n",
      "2018-10-14 10:29:00                 1.65                   0.27   239.00   \n",
      "\n",
      "                     global_intensity  sub_metering_1  sub_metering_2  \\\n",
      "timestamp                                                               \n",
      "2018-09-07 10:34:00               0.8               0               0   \n",
      "2018-07-06 04:17:00               4.0               0               0   \n",
      "2018-09-10 10:05:00               1.8               0               0   \n",
      "2018-08-04 15:15:00               1.4               0               2   \n",
      "2018-10-14 10:29:00               7.0               0               2   \n",
      "\n",
      "                     sub_metering_3                 __dt  cost  \n",
      "timestamp                                                       \n",
      "2018-09-07 10:34:00               1  2018-09-07T10:34:00   1.5  \n",
      "2018-07-06 04:17:00              11   2018-07-06T4:17:00  16.5  \n",
      "2018-09-10 10:05:00               1  2018-09-10T10:05:00   1.5  \n",
      "2018-08-04 15:15:00               1  2018-08-04T15:15:00   4.5  \n",
      "2018-10-14 10:29:00              18  2018-10-14T10:29:00  30.0  \n"
     ]
    }
   ],
   "source": [
    "print(\"#####Examine the dataset\")\n",
    "print(\"dataset shape \",dataset.shape)\n",
    "print(dataset.sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Explore the data</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAGfCAYAAABhicrFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeZgcZb328e9NWMKOAi5sBhEXFAgSEQQUFBVFQAWNICrqEeWIuG8vChEXRFFEcYseFkUWcSOIngTZQZaEsASQXTwgnqOIsq/J/f5RzyRNM5mZznS6qmfuz3X1NV1LV93dmcyvn6eqnpJtIiIion7L1B0gIiIiKinKERERDZGiHBER0RApyhEREQ2RohwREdEQKcoRERENkaIcERHRECnKERERDZGiHH1J0gRJJ9SdIyKim1KUoy/Zng+sLWn5urNERHTLsnUHiBiF24CLJM0AHhiYafubtSWKiBiFFOXoZ3eWxzLAqjVniYgYNeWGFNHvJK1s+4Hh14yIaLYcU46+JWkbSdcBfyrTm0v6Xs2xIiKWWIpy9LNvAa8F/glg+yrg5bUmiogYhRTl6Gu2b2+bNb+WIBERXZATvaKf3S7pZYDLpVEHUrqyIyL6UU70ir4laS3gKGAnQMAs4MO2/1lrsIiIJZSiHCMmaQJwoO0j684CIGmi7YfrzhER0S0pytERSefa3qHuHACSbgb+D7gAOB+4yPY99aaKiFhyKcrREUlfBlYHTuGJo2jNrSnPBsD2wLbA64F/255cR5aIiNFKUY6OSDpnkNm2/coasqxHVZBfAWwO3A1caPuwXmeJiOiGFOXoW5IWALOBr9g+re48ERGjleuUoyOSni7pvyT9vkxvIum9NcXZAvgJsLekiyX9pMYsERGjlpZydKQU42OBg2xvLmlZ4Arbm9aUZxVgO6pu7H2outIn1ZElImK00lKOTq1l++fAAgDbj1PTKFqS5gAXA28CrgdenoIcEf0sI3pFpx6QtCZgAElbA3VdhvQ62/+oad8REV2Xohyd+jgwA9hI0kXA2sCeNWV5VNI3WXQTivOAQ3OtckT0qxxTjo6V48jPoxra8gbbj9WU45fANcDxZdY7gM1tv7mOPBERo5WiHB2RNDB61gVUI2jdV2OWK9sHChlsXkREv8iJXtGpdwE3AHsAf5Q0R1JdY2E/JGm7gQlJ2wIP1ZQlImLUckw5OmL7VkkPAY+Wx47AC2qKsz9wvKTVqbrS76b60hAR0ZfSfR0dkXQLcBdwIlUX9pW2F9ScaTUA2/fWmSMiYrRSlKMjkj5MNVjH+lTXBp8HnG/7lhqyrAkcUvIYuJDq7OvcTzki+lKKciyRMpLWu4FPAOvZnlBDhjOpTjo7ocx6O7CD7Z16nSUiohtSlKMjkr5B1TJdBbiEcia27VtryHK57S3b5s2xPaXXWSIiuiEnekWnLgG+Zvv/6g4CnCPpbcDPy/SewBk15omIGJW0lKNjknajZRQt26fXlOM+YGUWjb09AXigPLft1erIFRGxpFKUoyOSDgO2An5WZu0FzLH92fpSDU7SC21fW3eOiIiRSlGOjki6Gpg8cBmUpAlUt27crN5kTyZpru0X150jImKkMqJXLIk1Wp6vXluK4anuABERnUhRjk4dBlwh6ThJxwOXA1+pOdPi9KwbSNIRkl7Yq/31M0m/7/H+VpN0mKSfStq7bdn3epzlGZK+L+m7ktaUNE3SPEk/l/TMXmaJZkr3dXSs/PF4SZm8zPb/1plncXrZfS3pP6iu214WOBY4aTzfQlLS4j53Ab+13bMCVO4mdhPVlQPvAR4D9rb9SK8PcUj6b6orBFYG9qY6N+MkYHdgJ9u79ypLNFOKcnRM0ptpGUXL9q9rjjQoSZfY3rrH+3weVXHeC7gI+JHtc3qZoQkkzaca7W2wQwhb216xh1mecOcwSQcBrwd2A87scVG+wvYW5fn/2N5gcTljfMp1ytGR0t33HKpv9wDvl7ST7Q/WkEVUo3g92/ahkjYAnmH7MoAaCvIE4PnlcRdwFfAxSe+3/bZeZmmAPwHvt31T+wJJt/c4ywqSlhk4OdH2lyXdQTXwzSo9ztJ6yPAnQyyLcSpFOTr1CuBFLl0s5bjyvJqyfA9YALwSOBS4D/gli7rWe0bSN4FdgbOBrwx8MQAOl3RDr/M0wDQWX2Q+1MMcAKdT/Y78YWCG7eMl/R/wnR5nOU3SKrbvt/25gZmSngPc2OMs0UDpvo6OSPoV8FHbfynTzwK+anuvGrLMtf3iti7Bq2xvXkOW9wAn235wkGWrj+fjy0OR9C7bx9edA5IlmiHdJdGpNYE/STpX0rnAdcDakmZImtHjLI+VLuOBVvvaVC3nOry9vSBLOgsgBXlIH647QItkidql+zo6dXDdAVp8G/g18DRJX6Ya+/pzQ7+kuyRNBFYC1pL0FBad2LQasE4vs/SpJl1LnixRuxTl6Ijt84ZaLuli29v0KMvPJF0OvIrqj9gbbf+pF/tu8X7gI1QFeG7L/HuB7/Y4Sz9q0vGzZInapShHt03s1Y4kHQWcYru24mf7KOAoSR+y3euThsaCJrUIkyVql6Ic3dbLb/hzgc9Jei5VN/Yptuf0cP9IeqXts4G/luu3n8D2r3qZpw9dVHeAFskStcvZ19FVddwEQtJTgT2AtwEb2N64h/v+gu1DJB07yGLbfk+vsjSRpI8NMvse4HLbVyZL/VmiWVKUo6taL0/q4T63AqYCbwSus71rL/cfiyfpRGAK1bXCALsAs6kGWDnV9teSpd4s0SwpytGxcm3yxrb/IGlFYFnb95VlL7J9TY9yHA68GbgF+DnwK9v/7sW+B8nyYaoxr+8DfgS8GPiM7Vl15GkKSTOBPWzfX6ZXAX4BvImqVbhJstSbJZol1ylHRyS9j+qPxw/LrPWA3wws71VBLv4MbGN7Z9vH1FWQi/fYvhd4DfA0qvGvv1pjnqbYAHi0Zfox4Fm2HwIeSZZGZIkGyYle0akPAlsBlwLYvknS03oZQNLzbV8PXAZsUMa8Xsj23MFfuXRjlZ+vB461fVUZm3u8OxG4RNJpZXpX4CRJK1MNPJMs9WeJBkn3dXRE0qW2Xzpw7FjSssBc25v1MMN02/tJGuzuS7b9yl5lacl0LLAusCGwOTABONf2lr3O0jSSpgDbUn1xubDXZ8gnS/STFOXoiKSvAf8G3kl1Y4H/pDq56qAasky0/fBw83qUZRlgMnCr7X9LWhNY1/bVvc7SNGUo1KfT0jNn+3+SpTlZojlSlKMjpfi8l+rYqYCZwI9dwy/SYJdf1XFJVsu+1wWexRP/yJ5fR5amkPQh4BDg/4D5VL8z7mXPSrJEP8kx5ejU7sBPbP+orgCSnkHVVbyipC144njTK9WU6XCqy7Kuo/ojC9VAKuO6KFPdWOF5tv9ZdxCSJfpAinJ0ajfgW5LOB04GZtp+vMcZXgvsS3Xm9zdYVJTvBf5fj7MMeCPVH9mcOftEt1MNitEEyRKNl+7r6Jik5YDXUbUMtwPOtP0fNeTYw/Yve73fwUj6PfCWgetOoyLpv4DnAWfQcqmP7W8mSzOyRLOkpRwds/1YKUIGVqTq0u55UQa2lHTWwPXJ5daJH7fd09s3Fg8CV5Z7KLf+kT2whixN8j/lsXx5JEvzskSDpKUcHZG0M9UY0zsC5wKnALNq6MIedEjPuk70kvSuwebbPr7XWSKif6WlHJ3al+pY8vsbcPx0gqQVBnKUIT9XqCOI7ePL/jewfUMdGZpE0rdsf0TS6Qxy5zDbuyVLvVmimVKUoyO231Z3hhYnAGeVgTsMvAeopWUqaVfgCKquyA0lTQYOHcd/ZH9afh5Ra4pKskTfSPd1jIikC21vJ+k+nvgNf+D6ytVqyvU64FUlxyzbM2vKcTnwSqpRvLYo8+bZ3rSOPE1QBsc43vY+ydLMLNE8aSnHiNjervxcte4srWz/Hvh93TmAx23f0zbc9bj+xmt7vqS1JS1v+9HhX5EsESnK0RFJP7X9juHm9SjL1sB3gBdQdRtPAB6oqdV+jaS9qY5zbwwcCPyxhhxNcxtwkaQZwAMDM2u69CdZovFSlKNTL2ydKDekqOumC0dTnQl+KtUN498JPKemLB8CDqK6HOpEquFHv1hTlia5szyWAeruZUmWaLwcU44RkfRZqtGyVqS6Jheq47iPAtNtf7aGTHNsT5F09cCYwZL+aPtlNWR5i+1Th5s3Xkla2fYDw6+59CVLNNkydQeI/mD7sHI8+eu2VyuPVW2vWUdBLh6UtDzVoB1fk/RRYOWasgz2GdT1uTSGpG0kXQf8qUxvLul7ydKcLNEsKcrRqcskrT4wIWkNSW+sKcs7qH6HD6A6Lrc+sEcvA0h6naTvAOtK+nbL4zig5wOqNNC3qMYq/yeA7auAlydLo7JEg+SYcnTqENu/Hpgo9w4+BPhNr4PY/ksZsOOZtr/Q6/0XdwJzqG7UcXnL/PuAj9aSqGFs3952Vvr8xa2bLDHepShHpwbrXanl96gJA3aUFs5Vkk60/Viv9ttHbpf0MsDlUMOBlC7bZGlMlmiQdF9Hp+ZI+qakjSQ9W9KRPLGF2EvTgK2AfwPYvhKYVFOWrSSdKelGSbdK+rOkW2vK0iQfAD5Idf/rO4DJwH8mS6OyRIOkpRyd+hDweaobUQiYRfXHpQ6DDdhRl/+i6q6+nHRDtnqe7be3zpC0LXBRsjQmSzRILomKvlXuSXsW8BmqE7wOBJaz/YEaslxq+6W93m/TDXbXrhrv5JUs0XhpKUdHJK0NfIpqEJGJA/Ntv7KGOIMN2PGlGnIAnCPp68CveOL9lOfWlKdWkrYBXgasLeljLYtWoxp5LVlqzhLNlKIcnfoZVdf1G6iOi70L+EevQ5RB/b9g+5NUhbluA63kKS3zTHWTivFoeWAVqr8xrSNW3QvsmSyNyBINlO7r6Iiky21v2TaK1nm2X1FDlrNraqHHCEl6Vrl0rfaRq5Il+kFaytGpgct+/iZpF6rrdNerKcsVZUD/U3nioP6/6lUASfvYPqGtK3Kh3GCAdST9nqp1uIGkzYH3267jTONkicZLUY5OfamM6PVxqjs0rUZ9g2Q8lWpEpNbWsqmO6/bKwLCejbipgKSVqP5tNrD9vnLHqufZ/m1NkQZGrpoB1XXdkuoeRStZorFSlKMjLX/c7wF2bF8u6bO2D+tRlncPtbwXWWz/sPwcckSxHn4ux1JdlrVNmb6DqiehrqLcqJGrkiWaLoOHRLe9pe4ALcZjlo1sf41ymMH2Q1TXk9flCSNXSfoEDRlFK1miiVKUo9saMZJHMR6zPFrGAzeApI1ouUSrBoONXFXXYDPJEo2X7uvotiadzj8esxwC/DewvqSfAdsC+/Zo309i+y7g7cOu2APJEv0gRTm6bTy2TkeiJ1lsnylpLrB12eeHSwGohaQNqQZ5mUTL35te3jQkWaKfpChHt51ad4AW4y6LpIFhGv9Wfm5Qzpb/i+067u/8G6pxwU8HFtSw/2SJvpLBQ2JEJH2HIbpgbR+YLPVmAZB0CfBi4GqqlvKLyvM1gQ/YntXjPI0ZEzxZoh+kpRwjNafuAC2SZfFuA95r+1oASZsAnwS+SHX9dk+LMnCUpEPKfuseEzxZovHSUo4YQyRdaXvyYPMGW9aDPIcB7wBuYVE3resYHjVZoh+kpRwdKXeJ+jSwCTXfJSpZBnWDpO8DJ5fpqcCNklZg0RCpvfQm4Nm2H61h3+2SJRov1ylHp35GNcjBhsAXqLpLZydLY7LsC9wMfIRq+NNby7zHGGQEth64Clijhv0OJlmi8dJ9HR1p2F2ikqXhJJ0LbEb1BaX12GkdlyElSzReuq+jU026S1SytCk3oDiMJ3ejP7vXWYpDatrvYJIlGi8t5eiIpDcAFwDrs+guUV+wPSNZ6s8i6UKqP/hHArsC76b6f97IIiDpYtvbDL/m0pcs0QQpyhFjSEs3+jzbm5Z5F9jevu5sg5F0he0t6s4ByRLNkBO9oiOSni3pdEl3Sfq7pNMk1dI1miyDeljSMsBNkg6Q9CbgaTXkGKkmtQrGXRZJT7p72WDzondSlKNTJwI/B54BrEM1fORJydKYLB8BVgIOBLYE9gHeWUOO6A+fHeG86JGc6BWdku2ftkyfIOmAZGlMlkm2ZwP3Ux1PHmj5XFpDlpEYdzcNGaGlmkXS64DXA+tK+nbLotWAOsZIjyLHlGNEJD21PP0U8G+qwSlMNTjFCra/mCz1Zil55tp+8XDzepzpGcBWVJ/LbNv/27LsRbavSZbeZpG0OdU9nA8FDm5ZdB9wju1/La19x9BSlGNEJP2Z6o/HYN/g3ctLbpJl0BwDLZ+3Aqe0LFoN2MT2Vr3IMUiu/6D6o3821Wf0CuBQ28ckS/1ZJC1n+7Hy/CnA+rav7mWGeKIU5YgxoLR8tqAaTawxLR9JNwAvs/3PMr0m8Efbz0uW+rOUQUx2ozqUeSXwD+A82x/rZY5YJMeUoyOSlgP2B15eZp0L/HDg23ay1JPF9lXAVZJOqOm+yYtzB9UXgwH3AbcnS2OyrG773tJyP9b2IZLSUq5RinJ06vvAcsD3yvQ7yrz/SJb6skiaR7mMRnpyT/rA0J+9ImmgpfVX4FJJp1Hl2x24LFnqz1IsK+mZVIc9Dqph/9EmRTk69RLbm7dMny3pqmSpPcsberivkVi1/LylPAacliyNyQLViV4zgYtszy7X1t9UU5Ygx5SjQ5LmAm+xfUuZfjbwizrO7k2WxWZ5OvCSMnmZ7b/3OkNELJm0lKNTnwTOkXQr1Vmjz6JcD5ss9WeR9Fbg61THtAV8R9Inbf+i11lKnnMYZHSqmu55nSxPzrEe1Vjt25Y8FwIftn1HL3PEImkpR8ckrQA8j+qP/vW2HxnmJcnSuwxXAa8eaB1LWhv4Q1vXei/zbNkyORHYA3jc9qeSpf4sks6kGo1uYOCbfYC32351L3PEIinKMSKS3jzUctu/SpZ6s0B1wtfAjSjK9DLAVa3z6tak+0yP9yySrrQ9ebh50Tvpvo6R2rVteuDbnMrzXhafZFm830uayaJxt6cCv+txhoVaRjyDaqz9KVTjgydLM7LcJWkfFv2+7AX8s4YcUaSlHB2R9HGeOIKVgXuAy21fmSz1ZimX3PyDaghFARfY/nWv9j9IntYRzx4DbqMauerCZKk/i6QNgKOBbUqePwIH2v6fXuaIRXKXqOjUlsAHgGdS3Q1pP2AH4EeSen1sLlmebFXgM1RjKt9C9Ue2Tp8GJtvekOq45QPAg8nSmCxfBN5le23bTwPeA0yrIUcMsJ1HHiN+UF3TuErL9CrAfwMrAtclS/1Zyv43A74MXE91olddvy9Xl5/bAedTDZJxabI0IwtwxUjm5dG7R1rK0akNgEdbph8DnmX7IaDXZxsny+L9HfhfquODT6th/wPml5+7AD+wfRqwfLI0Jssy5UYUwMJj3TnXqEb58KNTJwKXlOEBoTrR6SRJKwPXJUu9WSTtT3Vy19rAL4D32e71Z9Hqr5J+COwEHF4uG6urMZAsT/YN4I+SfkF1TPmtVD0sUZOc6BUdK9dYbkd1ksqFtuckSzOySPoqcLJ7fKLb4khaCdgZmGf7pjLO8qa2ZyVLY7JsAryS6vf2rJq/xI17KcoRERENkWPKERERDZGiHEtM0n51ZxiQLINLlsEly+CalKVuko6R9HdJ1yxmuSR9W9LNkq6W1JWbz6Qox2g06T9wsgwuWQaXLINrUpa6HUd13H9xXgdsXB77Ud0/fdRSlCMiItrYPh+4e4hVdgd+4solwBrlhL1RySVR49RaT53gSesvN6ptbLDuskzZfOKozxScd/fao90Ey67xFFZYf/1RZ9n0qf8YdZZufS43Xr3SqLNMZCVW01NHleWxjSaOOgfAcmuvzorPWWdUWRY83p12xIQ112CFSeuNKssaK3VnAK5Vn7EST99kdP9GT1n2ga5kefo6y/L8zVYYVZYb5j16l+3R/6ceodfuuLL/eff84Vdsc/nVj1wLPNwya7rt6R1sYl3g9pbpO8q8v3UcpkWK8jg1af3luGzm+nXHAGDjE/avO8JCl+3TlR6ornjtulvUHQGAO7/xgrojLHT/3aP/otItb548t+4IC735KbVdCfgkL9/w1r/0cn//vHs+l83coOPXTXjmTQ/bnjKKXWuQeaP+Mp6iHBERfcvAAhbUses7gNaWzXrAnaPdaI4pR0REHzPzvaDjRxfMAN5ZzsLeGrjH9qi6riEt5YiI6GNVS7n7g2BJOonqTm9rSboDOARYDsD2D6juU/564GaqO3y9uxv7TVGOiIi+tjS6r23vNcxyAx/s9n5TlCMiom8ZM38MDRedohwREX1taXRf1yVFOSIi+paB+WOoKOfs64iIiIZISzkiIvpauq8jIiIawJATvSIiIpqilvG8lpIU5YiI6FvGY+pErxTliIjoX4b5Y6cmL/2zryUdJ2nPYda5TdJaHWxzX0lHjz7dwu39v7bpP3Zr2xERsfRUw2x2/miqXBJVeUJRtv2yuoJ0i6T0gkTEOCDmL8GjqbpalCV9XtL1ks6UdJKkT7Qtf5WkKyTNk3SMpBVaFn9S0mXl8Zyy/q6SLi2v+YOkp48wx6Cvk7SKpGPL/q+WtIekrwIrSrpS0s/KeveXn6dIen3Ldo8rr5kg6euSZpftvH+ILDtIOl/SryVdJ+kHkpYpy/YqWa6RdHiZ91ZJ3yzPPyzp1vJ8I0kXludbSjpP0uWSZkp6Zpl/rqSvSDoP+PBIPquIiH5mYIE7fzRV14qypCnAHsAWwJuBKW3LJwLHAVNtb0p1PLv17vb32t4KOBr4Vpl3IbC17S2Ak4FPjTDO4l73earba21qezPgbNufAR6yPdn229u2czIwteRfHngV1Z1B3lu28xLgJcD7JG04RJ6tgI8DmwIbAW+WtA5wOPBKYDLwEklvBM4Hti+v2x74p6R1ge2ACyQtB3wH2NP2lsAxwJdb9rWG7VfY/kZ7CEn7SZojac4//jl/iLgREf1jLLWUu9nFuR1wmu2HACSd3rb8ecCfbd9Ypo+nusPGQAE+qeXnkeX5esAppSW4PPDnEWZZ3Ot2At42sJLtfw2znd8D3y4t+p2B820/JOk1wGYtx8pXBzYeIt9ltgdavCdRfVaPAefa/keZ/zPg5bZ/U1r0q1LdQPtE4OVUBfpXVJ/ji4AzJQFMAFrv4XnK4t6M7enAdIApm09s8HfFiIiRqYbZbG6R7VQ3u6+H+1SGW+5Bnn8HOLq0rN8PTBxhlsW9Tm37GTqQ/TBwLvBaqhbzyS3b+VBpXU+2vaHtWUNtapDpoT6Pi6nuzXkDcAFVQd4GuKi87tqWfW9q+zUtr31gZO8uImJsWGB1/GiqbhblC4FdJU2UtAqwS9vy64FJA8eLgXcA57Usn9ry8+LyfHXgr+X5uzrIsrjXzQIOGJiQ9JTy9LHSLTyYk6kK5PbAzDJvJrD/wGskPVfSykPk2UrShuVY8lSqz+pS4BWS1pI0AdiLRZ/H+cAnys8rgB2BR2zfQ1Wo15a0Tdn3cpJeOMS+IyKiT3StKNueDcwArqLqZp0D3NOy/GGq4naqpHlUZ6X/oGUTK0i6lOoEpY+WedPK+hcAd3UQZ3Gv+xLwlHJi1VVUxQ6qLt2rB070ajOLqvv4D7YfLfN+DFwHzJV0DfBDhj4UcDHwVeAaqi7uX9v+G/BZ4Byqz2yu7dPK+hdQdV2fb3s+cDtVIadk2BM4vLyHK4G+P1s8ImJJDHRf55jy4I6wPU3SSlStvG/Y/tHAQttnUZ0I9gS2J5WnX2ibfxpw2iDrH0d10tighnjd/QzS4rb9aeDTLdOrtDx/DFizbf0FVJdRPeFSqiE8aHtq+0zbJ1IdM26ffwst3dtt3dPYvpLqi0L763YYYZ6IiDHBiPlj6Orebhfl6ZI2oTqGe7ztuV3efkRExBM0+Rhxp7palG3v3c3tDUfSQcBb2mafavvLg62/lLNsCvy0bfYjtl9KdbJYRER02Vg7+7qvR30qxbfnBXgwtudRXW8cERE9I+Y73dcRERG1q8a+TlGOiIhohHRfR0RENICd7uuIiIjGWJCWckRERP2qs6/TUo6IiGiAdF9HREQ0wlg7+3rsvJOIiIg+l5byODXv7rXZ+IT9644BwE37fL/uCAttdPIH6o6w0Lq7L6g7AgDL/qE5fyZWGepebD0288at646w0O9WaE4W+FjP9zh/DA2zmZZyRET0rYEbUnT6GI6knSXdIOlmSZ8ZZPkGks6RdIWkqyW9vhvvpzlfgSMiIpbAgi6f6FXucf9d4NXAHcBsSTNsX9ey2ueAn9v+frkR0++ASaPdd4pyRET0raV0SdRWwM22bwWQdDKwO9BalA2sVp6vDtzZjR2nKEdERN8yWtJjymtJmtMyPd329PJ8XeD2lmV3AC9te/00YJakDwErAzstSYh2KcoREdHXlvCSqLtsT1nMssGqvNum9wKOs/0NSdsAP5X0ItujOkMzRTkiIvqWzdIYPOQOYP2W6fV4cvf0e4Gdqwy+WNJEYC3g76PZcc6+joiIPiYWLMFjGLOBjSVtKGl54G3AjLZ1/gd4FYCkFwATgX+M9t2kpRwREX3LdL+lbPtxSQcAM4EJwDG2r5V0KDDH9gzg48CPJH20xNjXdnsXd8dSlCMioq8tjRtS2P4d1WVOrfMObnl+HbBtt/ebohwREX3LiAUZ0SsiIiK6LS3liIjoa7mfckRERAOY7g+zWacU5YiI6GNi/vCXOPWNJf56Iek4SXsOs85tktbqYJv7Sjp6STMtqbLfdVqmf1wGGI+IiAYbaCl3+miqvm0pS1rW9uNd2ty+wDWUEVts/0eXtlubLn8+ERGNNe5aypI+L+l6SWdKOknSJ9qWv6rcU3KepGMkrdCy+JOSLiuP55T1d5V0aXnNHyQ9fYQ5jpP0TUnnAIdLWrnsb3bZ1u5lvUmSLpA0tzxe1rKNT5WcV0n6amntTwF+JulKSStKOlfSFEn7S/pay2v3lfSd8nyf8p6ulPTDcquvxeW+X9I3SpazJK1d5k+WdEm5F+evJT1F0tMkXV6Wby7JkjYo07dIWknS2pJ+Wd73bEnbluXTJE2XNAv4yQlwlZ0AACAASURBVEg+04iIfmZrTLWUh00maQqwB7AF8GaqAta6fCJwHDDV9qZUre/9W1a51/ZWwNHAt8q8C4GtbW8BnAx8qoPMzwV2sv1x4CDgbNsvAXYEvi5pZaqxR19t+8XAVODbJevrgDcCL7W9OfA1278A5gBvtz3Z9kMt+/pFec8DpgKnlCHVpgLb2p4MzAfePkTmlYG5Jc95wCFl/k+AT9veDJgHHGL778BESasB25ds20t6FvB32w8CRwFHlve9B/Djln1tCexue+/2EJL2kzRH0pwF9z8wRNyIiP4x38t0/GiqkXRfbwecNlCsJJ3etvx5wJ9t31imjwc+yKICfFLLzyPL8/WoitszgeWBP3eQ+VTb88vz1wC7tbTcJwIbUHVDHy1poGA+tyzfCTi2FDZs3z3Ujmz/Q9KtkrYGbirv9aLy/rakuvE1wIoMPQj5AuCU8vwE4FeSVgfWsH1emX88cGp5/keqkWJeDnyFatBzARe0vI9Nyr4BVpO0ank+o+2LRev7mQ5MB1hh/fVHPRxcRETdDCMZy7pvjKQoD/duh1vuQZ5/B/im7RmSdqC6L+VItTbxBOxh+4YnBJKmAf8HbE7VG/Bwy/qdFqNTgLcC1wO/tm1V1fB425/tcFsDhstwAVUr+VnAacCny2t+W5YvA2zTXnxLkU4TOCLGETW65dupkbyTC4FdJU2UtAqwS9vy64FJA8eLgXdQddEOmNry8+LyfHXgr+X5uzpOvchM4EOlSCJpi5bt/63c1/IdVAOKA8wC3iNppbL+U8v8+4BVGdyvqLq892JRa/csYE9JTxvYTuleXpxlgIEz1fcGLrR9D/AvSduX+a2f2/nAPsBN5T3cDbyeqpU+8D4OGNh46RGIiBh3qrOv1fGjqYZtKdueLWkGcBXwF6pjnPe0LH9Y0ruBUyUtS3XLqx+0bGIFSZdSFaa9yrxpZf2/ApcAGy5h/i9SdZNfXQrzbcAbgO8Bv5T0FuAcSuvR9n+XAjZH0qNUg43/P6pj4j+Q9BCwTdv7/5ek64BNbF9W5l0n6XPALEnLAI9RdWn/ZTE5HwBeWE7guodFX1TeVfa7EnAr8O6y/dvK94zzy3oXAuvZ/leZPhD4rqSrqf4Nzwc+0MkHFxExVoylEb00kjtNSVrF9v2leJwP7Gd77lJPN0ZIut/2KnXnaLXC+ut7vQ9/tO4YANy0z/frjrDQRic357vNuucvqDsCAPet25wrJx9fue4EizSpsTV/heHX6ZUbv/Cxy21PGX7N7njmC5/id530qo5fd/jmv+xpzpEa6f+26aoG05hIdSw1BTkiImo31u4SNaKiPNjlNUuTpIOAt7TNPtX2l3uZo1Olm779O+s7mtZKjogYSxaMoe7r5vRLtSjFt9EFeDC2X1p3hoiI8cSG+eOtpRwREdFU4677OiIioomqY8rpvo6IiGiEsXRDihTliIjoWwODh4wVKcoREdHH0n0dERHRGOPthhQRERGNlEuiIiIiGiTd19H3Nn3qP7isIWNON2m86Vve9oPhV+qR136sGTf/uuc3L6g7wkL3392cwa932XRe3REW2mvNi4dfqUde/oW6E/S3FOWIiOhb43Ls64iIiKbKiV4RERENMNauUx47R8cjImJcWuBlOn4MR9LOkm6QdLOkzyxmnbdKuk7StZJO7MZ7SUs5IiL6l7t/TFnSBOC7wKuBO4DZkmbYvq5lnY2BzwLb2v6XpKd1Y99pKUdERN8y1THlTh/D2Aq42fatth8FTgZ2b1vnfcB3bf8LwPbfu/F+UpQjIqKvLSit5U4ew1gXuL1l+o4yr9VzgedKukjSJZJ27sZ7Sfd1RET0rVGc6LWWpDkt09NtTy/PB9ug26aXBTYGdgDWAy6Q9CLb/16SMK0bjYiI6FtLWJTvsj1lMcvuANZvmV4PuHOQdS6x/RjwZ0k3UBXp2UsSZkC6ryMiom8NDB7S5e7r2cDGkjaUtDzwNmBG2zq/AXYEkLQWVXf2raN9P2kpR0REX+v24CG2H5d0ADATmAAcY/taSYcCc2zPKMteI+k6YD7wSdv/HO2+U5QjIiLa2P4d8Lu2eQe3PDfwsfLomhTliIjoXx5bI3qlKEdERN/KMJsNIek4SXsOs85t5QD8SLe5r6Sjh1j+AUnvHGYbkyW9fqT77CDboZJ2Ks8/Immlbu8jIqIfLYUTvWqTlnIHbI/kZruTgSm0HYvowr4Pbpn8CHAC8GA39xER0W/G2q0b+6KlLOnzkq6XdKakkyR9om35qyRdIWmepGMkrdCy+JOSLiuP55T1d5V0aXnNHyQ9fYQ5pg3sW9K5kg4v271R0vbl1PlDgamSrpQ0VdLKJdPssr/dy+v3lfQrSf8t6SZJXyvzJ5RegGvK+/lomX+cpD0lHQisA5wj6RxJ75V0ZEvG90n65mLy7ydpjqQ5//jn/BF++hERzWar40dTNb4oS5oC7AFsAbyZqhXaunwicBww1famVK3//VtWudf2VsDRwLfKvAuBrW1vQTWm6aeWMN6yZdsfAQ4pY6QeDJxie7LtU4CDgLNtv4TqmravS1q5vH4yMBXYlKqQr1/mrWv7ReX9HNu6Q9vfprqIfUfbO5b8u0larqzy7vbXtLx2uu0ptqesveaEJXzLERHNshTGvq5N44sysB1wmu2HbN8HnN62/HnAn23fWKaPB17esvyklp/blOfrATMlzQM+CbxwCbP9qvy8HJi0mHVeA3xG0pXAucBEYIOy7Czb99h+GLgOeBbVxefPlvSdMpbqvUMFsP0AcDbwBknPB5azPW8J309ERF+xx9Yx5X4oysN9esMt9yDPvwMcXVqi76cqlEvikfJzPos/Pi9gj9Jynmx7A9t/anv9wm2UO45sTlXAPwj8eAQ5fgzsyxCt5IiIsSrd1711IbCrpImSVgF2aVt+PTBp4Hgx8A7gvJblU1t+Xlyerw78tTx/V5fz3ges2jI9E/iQJAFI2mKoF5ezxZex/Uvg88CLh9uH7Uupxmndm0U9AxER48BSGWazNo0/+9r2bEkzgKuAvwBzgHtalj8s6d3AqZKWpRqztPUs6RUkXUr1BWSvMm9aWf+vwCXAhl2MfA6LuqsPA75IdSz76lKYbwPeMMTr1wWOlTTwhemzg6wzHfi9pL+V48oAPwcmD9zbMyJivGhyy7dTjS/KxRG2p5Vrc88HvmH7RwMLbZ9FdSLYE9ieVJ5+oW3+acBpg6x/HNVJY4OyPa3l+Q4tz++iHFO2fTfwkraXvn+4fdluLdRPah3b3rfl+XeouuBbbQccSUTEOJLBQ+oxvbQ85wK/tD237kBNIWkNSTcCD5UvJxER0af6oqVse+9e7k/SQcBb2mafavvLvcwxEuWG2s+tO0dERC1cnYE9VvRFUe61UnwbV4AjIuLJmnzdcadSlCMiom+ZnOgVERHREM2+xKlTKcoREdHXckw5IiKiIdJ9HRER0QB2inJERERj5JhyREREQ+SYckREREOk+zr63o1Xr8Rr1x3yhlU9s+7uC+qOsNBrPza57ggLzbzzyrojAPDqtzbnM1lrheY0if58w1p1R1joy/fuUHeEFrf2dG+m2bdi7FS/jH0dEREx5qWlHBERfa05/Sejl6IcERH9K5dERURENMgYaiqnKEdERF9LSzkiIqIhcp1yREREA4y1WzfmkqiIiOhfBqzOH8OQtLOkGyTdLOkzQ6y3pyRLmtKNt5OiHBERfa26KUVnj6FImgB8F3gdsAmwl6RNBllvVeBA4NJuvZcU5YiI6G9egsfQtgJutn2r7UeBk4HdB1nvi8DXgIdH/R6KFOWIiOhj1TCbnT6AtSTNaXns17LRdYHbW6bvKPMW7VXaAljf9m+7+W5yoldERPS3JTv7+i7bizsOPNhB54V7kbQMcCSw7xLteQhpKUdERDzRHcD6LdPrAXe2TK8KvAg4V9JtwNbAjG6c7JWiXANJkyRdU55PlvT6ujNFRPSlMszmEnRfD2U2sLGkDSUtD7wNmLFwl/Y9tteyPcn2JOASYDfbc0b7dlKU6zcZSFGOiFhSXT7Ry/bjwAHATOBPwM9tXyvpUEm7LZ03Uckx5S6RdDjwF9vfK9PTgPuAZ1CdVm/gS7ZPaXnN8sChwIqStgMOA/4MfAtYEXgIeLftGyStBBwHPJ/ql2QS8EHbcyS9BvgCsAJwS3nN/Uv7PUdENEP3Bw+x/Tvgd23zDl7Mujt0a79pKXfPycDUlum3AndRtYQ3B3YCvi7pmQMrlFPtDwZOsT25FOzrgZfb3qIs+0pZ/T+Bf9nejOo0/C0BJK0FfA7YyfaLgTnAxwYLKGm/gTMNH+ORLr3tiIiadf+SqNqkpdwltq+Q9DRJ6wBrA/+iKsgn2Z4P/J+k84CXAFcPsanVgeMlbUz1q7Ncmb8dcFTZ1zWSBraxNdXF7RdJAlgeuHgxGacD0wFW01Mb/GsZEdGBMfTXLEW5u34B7EnVZX0ysNESbOOLwDm23yRpEnBumb+4/hkBZ9reawn2FRHR3waG2Rwj0n3dXSdTnaW3J1WBPh+YKmmCpLWBlwOXtb3mPqrT6wesDvy1PN+3Zf6FVF3ilOHeNi3zLwG2lfScsmwlSc/t1huKiGi6bg+zWacU5S6yfS1Vgf2r7b8Bv6bqqr4KOBv4lO3/bXvZOcAmkq6UNJVqyLbDJF0ETGhZ73vA2qXb+tNlu/fY/gdV8T6pLLuE6mSwiIjxIceUY3Fsb9ry3MAny6N1nduoLjzH9t1Ux5lbtbZ0P19+PgzsY/thSRsBZwF/Kds4e5BtRESMD2Oo+zpFuX+sBJwjaTmq48j7l7O3IyLGNTW45dupFOU+Yfs+oCv364yIGDMa3h3dqRTliIjoYxpT3dc50SsiIqIh0lKOiIj+lu7riIiIhkhRjoiIaIgU5YiIiAYYY8NspihHRERfy3XKERERTTGGinIuiYqIiGiItJTHqcc2msid33hB3TEAWPYPzfk1vOc3zfhMAF791sl1RwDgzJ8fV3eEhaYcvH/dERa6+/nPqjvCQivt2n6fmxq9tve7TPd1REREU+REr4iIiAbI2NcRERENMoaKck70ioiIaIi0lCMioq/lRK+IiIimSFGOiIhoiBTliIiI+snpvo6IiGiOXKccERHREGkpR0RENEO6ryMiIppiDBXlDB4SERH9y4tO9urkMRxJO0u6QdLNkj4zyPKPSbpO0tWSzpLUlTuUpChHRER/8xI8hiBpAvBd4HXAJsBekjZpW+0KYIrtzYBfAF/rxltJUY6IiP7W5aIMbAXcbPtW248CJwO7P2GX9jm2HyyTlwDrdeOtjKmiLGmapE/0aF87SHrZErxuiqRvdznLWyRdK2mBpCnd3HZExBi1lqQ5LY/9WpatC9zeMn1Hmbc47wV+341QOdFrye0A3A/8caQvkLSs7TnAnC5nuQZ4M/DDLm83IqLxlvDs67tsL64RM9iFz4PuRdI+wBTgFUuUok3jW8qSVpZ0hqSrJF0jaaqk2yStVZZPkXRuy0s2l3S2pJskvW+I7e4g6TxJP5d0o6SvSnq7pMskzZO0UVlvbUm/lDS7PLaVNAn4APBRSVdK2n6w9crrp0maLmkW8JOy39+2LDtG0rmSbpV0YEu+z0u6XtKZkk4aqgfA9p9s3zCCz3K/gW+F8+99cLjVIyLGqzuA9Vum1wPubF9J0k7AQcButh/pxo77oaW8M3Cn7V0AJK0OHD7E+psBWwMrA1dIOsP2kz7MYnPgBcDdwK3Aj21vJenDwIeAjwBHAUfavlDSBsBM2y+Q9APgfttHlFwntq9Xtg2wJbCd7Yck7dCW4fnAjsCqwA2Svl9y7QFsQfVvNBe4fNhPahi2pwPTAVZ8zjpj6CKCiBjXuv/XbDawsaQNgb8CbwP2bl1B0hZUvZM72/57t3bcD0V5HnCEpMOB39q+QBpySLXTbD8EPCTpHKoD9r9ZzLqzbf8NQNItwKyWfe5Ynu8EbNKyz9UkrTrItoZab0bJNJgzyjesRyT9HXg6sF3L+0DS6UO94YiIcWspjH1t+3FJB1A1riYAx9i+VtKhwBzbM4CvA6sAp5a/+/9je7fR7rvxRdn2jZK2BF4PHFa6gR9nUdf7xPaXDDPdqrW7YUHL9AIWfTbLANu0F9VBvhgMtd4DI8wwv+x37AzkGhGxtC2Ffj/bvwN+1zbv4JbnO3V/r/1xTHkd4EHbJwBHAC8GbqPqEoaqm7fV7pImSlqT6mSs2aOMMAs4oCXP5PL0Pqou5+HWWxIXAruW97EKsMsothURMbZ1/5Ko2jS+KAObApdJupLqgPqXgC8AR0m6gKp12eoy4Ayq68a+OMTx5JE6EJhSRm25juoEL4DTgTcNnOg1xHodsz0bmAFcBfyK6mztexa3vqQ3SboD2AY4Q9LMJd13REQ/EUtnRK+69EP39Uyqfv12zx1k3WkdbPdc4NyW6R0GW2b7LmDqIK+/keqkslaDrTetbbp12+3LXtQyeYTtaZJWAs4HvjHEe/k18OvFLY+IGNMaXGQ71fiiPI5NL8O6TQSOtz237kAREY3T8JZvp8Z8UZa0KfDTttmP2H5pHXlGyvbe7fMkfRfYtm32UbaP7U2qiIgGSlHuH7bnAaM56aoxbH+w7gwREY0zhopyP5zoFRERMS6M+ZZyRESMbTmmHBER0RQpyhEREQ3Q8MFAOpWiHBERfS3d1xEREU2RohwREdEMaSlH31vw+DLcf/dKdccAYJWV606wyP13NyfMWis04y/NlIP3rzvCQnMO/X7dERba4kv/WXeEhe66Z5W6I9SrGf9VuiJFOSIi+ldO9IqIiGgGMbZuQJ+iHBER/S0t5YiIiGYYSyd6ZezriIiIhkhLOSIi+tsYaimnKEdERH9LUY6IiGgAj61jyinKERHR31KUIyIimiEt5YiIiKZIUY6IiGiGtJQjIiKaIGNfR0RENEiKckRERP3E2Oq+HlPDbEqaJukTPdrXDpJetgSvmyLp213O8nVJ10u6WtKvJa3Rze1HRDSal+AxDEk7S7pB0s2SPjPI8hUknVKWXyppUjfeypgqyj22A9BRUZa0rO05tg/scpYzgRfZ3gy4Efhsl7cfETFuSJoAfBd4HbAJsJekTdpWey/wL9vPAY4EDu/GvhtflCWtLOkMSVdJukbSVEm3SVqrLJ8i6dyWl2wu6WxJN0l63xDb3UHSeZJ+LulGSV+V9HZJl0maJ2mjst7akn4paXZ5bFu+EX0A+KikKyVtP9h65fXTJE2XNAv4Sdnvb1uWHSPpXEm3SjqwJd/nS+v3TEknDdUDYHuW7cfL5CXAeot5z/tJmiNpzvz7Hxj2s4+I6AeyO34MYyvgZtu32n4UOBnYvW2d3YHjy/NfAK+SNOpbO/fDMeWdgTtt7wIgaXWG/kayGbA1sDJwhaQzbN+5mHU3B14A3A3cCvzY9laSPgx8CPgIcBRwpO0LJW0AzLT9Akk/AO63fUTJdWL7emXbAFsC29l+SNIObRmeD+wIrArcIOn7JdcewBZU/0ZzgcuH/aQq7wFOGWyB7enAdIAVJq03ho7CRMS4tXTOvl4XuL1l+g7gpYtbx/bjku4B1gTuGs2O+6EozwOOkHQ48FvbFwzzZeQ02w8BD0k6h+obz28Ws+5s238DkHQLMKtlnzuW5zsBm7TsczVJqw6yraHWm1EyDeYM248Aj0j6O/B0YLuW94Gk04d6wwMkHQQ8DvxsJOtHRIwFS3ii11qS5rRMTy8NF6jOH2vXvpeRrNOxxhdl2zdK2hJ4PXBY6QZ+nEVd7xPbXzLMdKtHWp4vaJlewKLPZhlgm/aiOsgXg6HWG6qvuDXD/LLfjrtAJL0LeAPwKnv4vpmIiDFjyf7i3WV7ymKW3QGs3zK9HtDe4zqwzh2SlgVWp+p1HZV+OKa8DvCg7ROAI4AXA7dRdQlD1c3bandJEyWtSXUy1uxRRpgFHNCSZ3J5eh9Vl/Nw6y2JC4Fdy/tYBdhlqJUl7Qx8GtjN9oOj2G9ERN+RO38MYzawsaQNJS0PvA2Y0bbODOBd5fmewNndaBA1vqUMbAp8XdIC4DFgf2BF4L8k/T/g0rb1LwPOADYAvjjE8eSROhD4rqSrqT6v86lO8jod+IWk3amOPy9uvY7Zni1pBnAV8BdgDnDPEC85GlgBOLO0zC+xvUT7jojoO13uGyzHiA+gOjdoAnCM7WslHQrMsT0D+C/gp5Jupmohv60b+258UbY9k+qDaffcQdad1sF2zwXObZneYbBltu8Cpg7y+hupTiprNdh609qmW7fdvuxFLZNH2J4maSWqAv+NId7Lcxa3LCJiTFtK91O2/Tvgd23zDm55/jDwlm7vt/FFeRybXq6Lmwgcb3tu3YEiIhppDJ1FM+aLsqRNgZ+2zX7Edvvp7Y1ie+/2eZK+C2zbNvso28f2JlVERLOMtWE2x3xRtj0PGM1JV41h+4N1Z4iIaJwxdMHJmC/KERExto2llnLjL4mKiIgYL9JSjoiI/rV0htmsTYpyRET0NS2oO0H3pChHRER/S0s5IiKiGcbSiV4pyhER0b9MLomK/rfGSg/y5snNGCRs5o1b1x1hoV02nVd3hIX+fMNadUcA4O7nP6vuCAtt8aX/rDvCQld87nt1R1joJ/c243cF4N017DMt5YiIiKZIUY6IiKhfhtmMiIhoCjvHlCMiIpoiLeWIiIimSFGOiIhohrHUUs4NKSIiIhoiLeWIiOhfBhaMnaZyinJERPS3sVOTU5QjIqK/jaVjyinKERHR33KdckRERDOkpRwREdEEJseUIyIimqAa+3rsVOUU5YiI6G8L6g7QPWNq8BBJ0yR9okf72kHSy5bgdVMkfbvLWb4o6WpJV0qaJWmdbm4/IqLJZHf8aKoxVZR7bAego6IsaVnbc2wf2OUsX7e9me3JwG+Bg7u8/YiIZvISPhqq8UVZ0sqSzpB0laRrJE2VdJuktcryKZLObXnJ5pLOlnSTpPcNsd0dJJ0n6eeSbpT0VUlvl3SZpHmSNirrrS3pl5Jml8e2kiYBHwA+Wlqn2w+2Xnn9NEnTJc0CflL2+9uWZcdIOlfSrZIObMn3eUnXSzpT0klD9QDYvrdlcmUW8ysnaT9JcyTNeehfjwz5uUdE9Acvun1jJ4+G6odjyjsDd9reBUDS6sDhQ6y/GbA1VXG6QtIZtu9czLqbAy8A7gZuBX5seytJHwY+BHwEOAo40vaFkjYAZtp+gaQfAPfbPqLkOrF9vbJtgC2B7Ww/JGmHtgzPB3YEVgVukPT9kmsPYAuqf6O5wOVDfUiSvgy8E7inbO9JbE8HpgM8fZOnNve3MiJinGp8SxmYB+wk6XBJ29u+Z5j1T7P9kO27gHOArYZYd7btv9l+BLgFmNWyz0nl+U7A0ZKuBGYAq0ladZBtDbXeDNsPLSbDGbYfKXn/Djwd2K7lfdwHnD7Me8b2QbbXB34GHDDc+hERY4Xc+WNU+5OeWnoxbyo/nzLIOpMlXSzp2nLOz9SRbLvxRdn2jVQtzXnAYZIOBh5nUfaJ7S8ZZrpVax/ugpbpBSzqRVgG2Mb25PJYtxTKdkOt98AIM8wv+9UQ6w/nRKpWdkTE+ND77uvPAGfZ3hg4q0y3exB4p+0XUvX4fkvSGsNtuPFFuZxJ/KDtE4AjgBcDt1EVanhyAdpd0kRJa1KdjDV7lBFm0dLylDS5PL2Pqst5uPWWxIXAruV9rALsMtTKkjZumdwNuH4U+46I6B8GLej8MUq7A8eX58cDb3xSLPtG2zeV53dS9YSuPdyG++GY8qbA1yUtAB4D9gdWBP5L0v9v7+5CLSvrOI5/fxUTiA2OMzmeIL0IC3pD6PQiVogvFEEYglpJzliTiIV3wdDIMCTSQHlR5M0hal4kqIRwIunFARVLrUPNWF15NYZM5lHI6qKg+XextqfDmXW2e58ze++19/l+YLP3Ws+zn/Xfcy7+87ysZ30VeHpV/d8CPwMuAe7pM588qLuA+5M8Q/Pv9TjNIq+fAg8muZ5m/nmtekOrqt8lOQacBE4BizRzxWs5mOQdND38U+u9riRNpfX1fHckWVxxvNBbdzOInVV1url0nU5yUb/KST4AbKGZJu2r80m5qn5Bs2hqtbe31D0wRLuPAo+uOL6qraw313vWXEBvWP29q0631Tuw6nhl26vL3r3i8JtVdSDJeTQJ/r4+v8Xhakmb1/pGo5eqan6twiSPABe3FO0b5iJJ5oCjwK6qes0+eueT8ia2kOSdNHPmh6vq95MOSJK6aBSbgVTVtWteL3khyVyvlzxHMzTdVm8rzcjt3VX11CDXnfmknOQ9NP9LWenfVfXBScQzqKr67OpzSe4Hrlx1+ltV9f3xRCVJHTT++46PAbuAg733h1ZXSLIF+AlwpKp+PGjDM5+Uq+qPwEYWXXVGVX1p0jFIUqcUk9j7+iDwoyRfAJ4DboRmMyvgjqraA9wEfBTYnmR373u7q+pEv4ZnPilLkmZXGP9e1lX1EnBNy/lFYE/v8wPAA8O2bVKWJE23Dm+bOSyTsiRpus1QUu785iGSJG0W9pQlSdNrMgu9RsakLEmaauNe6DVKJmVJ0nQzKWvabXvDv7hh2+JrVxyDh9/4oUmHsOwz25+cdAjL7n3lqkmHAMB5n/zrpENYtvT38ycdwrIjr+yYdAjLbt26NOkQlt029iuek6c+dYZJWZI0vQqTsiRJneFCL0mSusGFXpIkdYVJWZKkDijgjElZkqQOcPW1JEndMUNJ2b2vJUnqCHvKkqTpNkM9ZZOyJGl6udBLkqSuKKjZ2T3EpCxJmm4OX0uS1AEOX0uS1CEz1FP2lqgZkuTyJJ+YdBySNFZVw786yqQ8Wy4HTMqSNpF1JGSTsgaV5NYkzyQ5meRokkuTHO+dO57kkl69G5P8qVfv8SRbgK8BNyc5keTmyf4SSRqDAs6cGf7VUc4pd0iSdwH7gCurainJhcBhvQeRzAAAAtpJREFU4EhVHU7yeeDbwKeA/cDHqur5JBdU1X+S7Afmq+rLa7R/O3A7wM63+KeXNCM63PMdlj3lbrkaeLCqlgCq6mXgCuAHvfKjwId7n38NHEryReD1gzReVQtVNV9V8xds908vaUY4fK0RCc1gTD8FUFV3AHcDbwVOJNk+4tgkSSNmUu6W48BNrybY3vD1b4BP98pvAZ7olb2tqp6uqv3AEk1y/gfwprFHLUkTU819ysO+OsqJxQ6pqj8nuRd4LMl/gT8AdwHfS/IV4EXgtl71byS5jKZ3fRw4CTwH7E1yAvh6Vf1w7D9CksapoNxmU6NSVYdpFnetdHVLvRtavv4y8P5RxCVJndXhnu+wHL6WJE23MS/0SnJhkl8lebb3vq1P3a1Jnk/ynUHaNilLkqZX1STuU94LHK+qy2imD/f2qXsP8NigDZuUJUnTbfy3RF3P/6cZD9PsHXGWJO8DdgK/HLRh55QlSVOt1tfz3ZFkccXxQlUtDPjdnVV1GqCqTie5aHWFJK8D7gM+B1wzaFAmZUnSFFt3z3epqubXKkzyCHBxS9G+Adu/E3i4qv6SZOCgTMqSpOk1oucpV9W1a5UleSHJXK+XPAf8raXaFcBHktwJnA9sSfLPquo3/2xSliRNufHfp3wM2AUc7L0/dFZIVbe8+jnJbprnEvRNyOBCL0nSFCugztTQrw06CFyX5Fngut4xSeaTfHcjDdtTliRpCFX1Ei2Lt6pqEdjTcv4QcGiQtk3KkqTpVTWJ4euRMSlLkqbaORiO7oxUh58rqdFJ8iJwaoPN7KB5QlUXGEs7Y2lnLO3ORSyXVtWbz0Uwg0jyc5q4h7VUVR8/1/FslElZ65Zksd99fuNkLO2MpZ2xtOtSLJuVq68lSeoIk7IkSR1hUtZGDLpP7DgYSztjaWcs7boUy6bknLIkSR1hT1mSpI4wKUuS1BEmZUmSOsKkLElSR5iUJUnqiP8B2rNM8jGteZoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cor_cols = [\"global_active_power\",\"global_reactive_power\", \"global_intensity\",\"voltage\",\"sub_metering_1\",\"sub_metering_2\",\"sub_metering_3\",\"cost\"]\n",
    "\n",
    "\n",
    "# plot correlation matrix\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.matshow(dataset.loc[:, cor_cols].corr(), fignum=1)\n",
    "plt.xticks(range(len(cor_cols)), cor_cols, rotation=90)\n",
    "plt.yticks(range(len(cor_cols)), cor_cols)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Correlation Plot</b>\n",
    "<p>The correlation plot, we can observe that Sub Meters 1 and 2 show some correlation with each other. Sub meter 3 has less correlation with Sub meter 1 and 2. Since cost is derived by aggregating Sub Meter 1, 2 and 3, we will retain only cost column to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "##updating data to dataset\n",
    "analysis_cols = [\"global_active_power\",\"global_reactive_power\", \"global_intensity\",\"voltage\",\"cost\"]\n",
    "\n",
    "# select the value columns in the DataFrame to compare\n",
    "dataset= dataset.loc[:, analysis_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load and parse the dataset and convert it to a collection of Pandas time series, which makes common time series operations such as indexing by time periods or resampling much easier. Here we want to forecast longer periods (one week) and resample the data to a granularity of every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = dataset.shape[1]\n",
    "data_kw = dataset.resample('2H').sum()\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_kw.iloc[:,i], trim='f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits\n",
    "\n",
    "Often times one is interested in evaluating the model or tuning its hyperparameters by looking at error metrics on a hold-out test set. Here we split the available data into train and test sets for evaluating the trained model. For standard machine learning tasks such as classification and regression, one typically obtains this split by randomly separating examples into train and test sets. However, in forecasting it is important to do this train/test split based on time rather than by time series.\n",
    "\n",
    "In this example, we will reserve the last section of each of the time series for evalutation purpose and use only the first part as training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 1 (changed 2 to 1) hour frequency for the time series\n",
    "freq = '2H' \n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 * 12  ##original 7*12\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 12 ##original 7*12 due to 2H sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we assume that the minimum date is the first day sensors started recording the dataset. We split the dataset 70, 30 with 70% of the data retained for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_active_power</th>\n",
       "      <th>global_reactive_power</th>\n",
       "      <th>global_intensity</th>\n",
       "      <th>voltage</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-30</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.89</td>\n",
       "      <td>15.4</td>\n",
       "      <td>963.23</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>65.51</td>\n",
       "      <td>6.69</td>\n",
       "      <td>279.0</td>\n",
       "      <td>14669.36</td>\n",
       "      <td>1129.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>31.56</td>\n",
       "      <td>4.52</td>\n",
       "      <td>133.6</td>\n",
       "      <td>12046.95</td>\n",
       "      <td>445.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-02</th>\n",
       "      <td>44.68</td>\n",
       "      <td>5.23</td>\n",
       "      <td>189.2</td>\n",
       "      <td>14428.37</td>\n",
       "      <td>598.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-03</th>\n",
       "      <td>63.28</td>\n",
       "      <td>6.93</td>\n",
       "      <td>271.8</td>\n",
       "      <td>16359.32</td>\n",
       "      <td>1021.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-21</th>\n",
       "      <td>37.94</td>\n",
       "      <td>4.12</td>\n",
       "      <td>159.2</td>\n",
       "      <td>12973.45</td>\n",
       "      <td>376.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-22</th>\n",
       "      <td>65.01</td>\n",
       "      <td>5.00</td>\n",
       "      <td>274.2</td>\n",
       "      <td>12747.78</td>\n",
       "      <td>666.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-23</th>\n",
       "      <td>67.17</td>\n",
       "      <td>6.40</td>\n",
       "      <td>286.0</td>\n",
       "      <td>13210.24</td>\n",
       "      <td>568.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-24</th>\n",
       "      <td>67.52</td>\n",
       "      <td>6.23</td>\n",
       "      <td>284.8</td>\n",
       "      <td>13704.01</td>\n",
       "      <td>1014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-25</th>\n",
       "      <td>60.69</td>\n",
       "      <td>5.56</td>\n",
       "      <td>255.2</td>\n",
       "      <td>15489.85</td>\n",
       "      <td>421.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            global_active_power  global_reactive_power  global_intensity  \\\n",
       "timestamp                                                                  \n",
       "2018-05-30                 3.45                   0.89              15.4   \n",
       "2018-05-31                65.51                   6.69             279.0   \n",
       "2018-06-01                31.56                   4.52             133.6   \n",
       "2018-06-02                44.68                   5.23             189.2   \n",
       "2018-06-03                63.28                   6.93             271.8   \n",
       "...                         ...                    ...               ...   \n",
       "2018-11-21                37.94                   4.12             159.2   \n",
       "2018-11-22                65.01                   5.00             274.2   \n",
       "2018-11-23                67.17                   6.40             286.0   \n",
       "2018-11-24                67.52                   6.23             284.8   \n",
       "2018-11-25                60.69                   5.56             255.2   \n",
       "\n",
       "             voltage    cost  \n",
       "timestamp                     \n",
       "2018-05-30    963.23    34.5  \n",
       "2018-05-31  14669.36  1129.5  \n",
       "2018-06-01  12046.95   445.5  \n",
       "2018-06-02  14428.37   598.5  \n",
       "2018-06-03  16359.32  1021.5  \n",
       "...              ...     ...  \n",
       "2018-11-21  12973.45   376.5  \n",
       "2018-11-22  12747.78   666.0  \n",
       "2018-11-23  13210.24   568.5  \n",
       "2018-11-24  13704.01  1014.0  \n",
       "2018-11-25  15489.85   421.5  \n",
       "\n",
       "[180 rows x 5 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailyGroups = dataset.resample('D').sum() ## Gathering unique days for dataset split\n",
    "dailyGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Confirm the datasets\n",
      "startTrainDate, maxDate, traingSplit, splitDate 2018-05-30 00:00:00 2018-11-25 00:00:00 126 2018-10-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "startTrainDate = dailyGroups.index.min()\n",
    "\n",
    "maxDate=dailyGroups.index.max()\n",
    "\n",
    "\n",
    "traingSplit=round(dailyGroups.shape[0]*.7)\n",
    "\n",
    "\n",
    "splitDate=dailyGroups.index[traingSplit]\n",
    "\n",
    "\n",
    "start_dataset = pd.Timestamp(startTrainDate, freq=freq)\n",
    "end_training = pd.Timestamp(splitDate, freq=freq)\n",
    "\n",
    "print(\"###### Confirm the datasets\")\n",
    "print(\"startTrainDate, maxDate, traingSplit, splitDate\", startTrainDate, maxDate, traingSplit, splitDate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case each time series just consists of a start time stamp (``start``) and a list of values (``target``). For more complex cases, DeepAR also supports the fields ``dynamic_feat`` for time-series features and ``cat`` for categorical features, which we will use  later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training - pd.Timedelta(1, unit='D')].tolist()\n",
    "         # We use -1 days, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As test data, we will consider time series extending beyond the training range: these will be used for computing test scores, by using the trained model to forecast their trailing 7 days, and comparing predictions with actual values.\n",
    "To evaluate our model performance on more than one week, we generate test data that extends to 1, 2, 3, 4 weeks beyond the training range. This way we perform *rolling evaluation* of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training + pd.Timedelta(k, unit='D') * prediction_length].tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the dictionary to the `jsonlines` file format that DeepAR understands (it also supports gzipped jsonlines and parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.9 ms, sys: 3.8 ms, total: 22.7 ms\n",
      "Wall time: 22.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data files locally, let us copy them to S3 where DeepAR can access them. Depending on your connection, this may take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=True):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-1-321286785020/iot-analytics-demo-notebook/output/train/train.json\n",
      "Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-1-321286785020/iot-analytics-demo-notebook/output/test/test.json\n",
      "CPU times: user 33.5 ms, sys: 3.93 ms, total: 37.4 ms\n",
      "Wall time: 248 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_output_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_output_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what we just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2018-05-30 00:00:00\", \"target\": [2.3, 1.15, 1.5500000000000003, 1.8800000000000001, 0.51,...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_output_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set with our dataset processing, we can now call DeepAR to train a model and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Here we define the estimator that will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-electricity-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set the hyperparameters for the training job. For example frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options (number of epochs, batch size, learning rate...). We use default parameters for every optional parameter in this case (you can always use [Sagemaker Automated Model Tuning](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"40\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"1E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test. This is done by predicting the last `prediction_length` points of each time-series in the test set and comparing this to the actual value of the time-series. \n",
    "\n",
    "**Note:** the next cell may take a few minutes to complete, depending on data size, model complexity, training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-23 15:49:46 Starting - Starting the training job...\n",
      "2020-10-23 15:49:48 Starting - Launching requested ML instances.........\n",
      "2020-10-23 15:51:20 Starting - Preparing the instances for training...\n",
      "2020-10-23 15:52:09 Downloading - Downloading input data...\n",
      "2020-10-23 15:52:21 Training - Downloading the training image...\n",
      "2020-10-23 15:53:01 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'1E-4', u'prediction_length': u'84', u'epochs': u'40', u'time_freq': u'2H', u'context_length': u'84', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'1E-4', u'num_layers': u'2', u'epochs': u'40', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'84', u'time_freq': u'2H', u'context_length': u'84', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Training set statistics:\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Real time series\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] number of time series: 5\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] number of observations: 7455\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] mean target length: 1491\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] min/mean/max target: 0.0/233.64217282/3125.33007812\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] mean abs(target): 233.64217282\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] contains missing values: no\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Small number of time series. Doing 128 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Test set statistics:\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Real time series\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] number of time series: 20\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] number of observations: 43000\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] mean target length: 2150\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] min/mean/max target: 0.0/240.197430834/3601.17993164\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] mean abs(target): 240.197430834\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] contains missing values: no\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] nvidia-smi took: 0.0252270698547 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:04 INFO 139658784810816] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 678.2748699188232, \"sum\": 678.2748699188232, \"min\": 678.2748699188232}}, \"EndTime\": 1603468385.214733, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468384.535409}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:05 INFO 139658784810816] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 1461.1968994140625, \"sum\": 1461.1968994140625, \"min\": 1461.1968994140625}}, \"EndTime\": 1603468385.99676, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468385.214823}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:06 INFO 139658784810816] Epoch[0] Batch[0] avg_epoch_loss=3.899795\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:06 INFO 139658784810816] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=3.89979457855\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:07 INFO 139658784810816] Epoch[0] Batch[5] avg_epoch_loss=4.308889\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:07 INFO 139658784810816] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=4.30888930957\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:07 INFO 139658784810816] Epoch[0] Batch [5]#011Speed: 307.53 samples/sec#011loss=4.308889\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:08 INFO 139658784810816] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}, \"update.time\": {\"count\": 1, \"max\": 2566.7948722839355, \"sum\": 2566.7948722839355, \"min\": 2566.7948722839355}}, \"EndTime\": 1603468388.563761, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468385.99685}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:08 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=241.523403767 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:08 INFO 139658784810816] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:08 INFO 139658784810816] #quality_metric: host=algo-1, epoch=0, train loss <loss>=4.26084656715\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:08 INFO 139658784810816] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:08 INFO 139658784810816] Saved checkpoint to \"/opt/ml/model/state_4a1bea8f-a00c-4705-87fc-314fed55b884-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 105.01503944396973, \"sum\": 105.01503944396973, \"min\": 105.01503944396973}}, \"EndTime\": 1603468388.669573, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468388.563933}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:09 INFO 139658784810816] Epoch[1] Batch[0] avg_epoch_loss=4.038434\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:09 INFO 139658784810816] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=4.03843402863\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:10 INFO 139658784810816] Epoch[1] Batch[5] avg_epoch_loss=4.049255\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:10 INFO 139658784810816] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=4.04925497373\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:10 INFO 139658784810816] Epoch[1] Batch [5]#011Speed: 237.14 samples/sec#011loss=4.049255\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:11 INFO 139658784810816] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2849.139928817749, \"sum\": 2849.139928817749, \"min\": 2849.139928817749}}, \"EndTime\": 1603468391.518888, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468388.66966}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:11 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=219.002228141 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:11 INFO 139658784810816] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:11 INFO 139658784810816] #quality_metric: host=algo-1, epoch=1, train loss <loss>=4.17254536152\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:11 INFO 139658784810816] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:11 INFO 139658784810816] Saved checkpoint to \"/opt/ml/model/state_3730a7ba-bbb2-4608-a14b-84313ea7c9c0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 83.18185806274414, \"sum\": 83.18185806274414, \"min\": 83.18185806274414}}, \"EndTime\": 1603468391.602719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468391.518987}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:12 INFO 139658784810816] Epoch[2] Batch[0] avg_epoch_loss=4.097916\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:12 INFO 139658784810816] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=4.09791612625\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:13 INFO 139658784810816] Epoch[2] Batch[5] avg_epoch_loss=4.109218\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:13 INFO 139658784810816] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=4.10921796163\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:13 INFO 139658784810816] Epoch[2] Batch [5]#011Speed: 299.46 samples/sec#011loss=4.109218\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] Epoch[2] Batch[10] avg_epoch_loss=4.028953\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=3.93263587952\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] Epoch[2] Batch [10]#011Speed: 306.68 samples/sec#011loss=3.932636\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2601.7019748687744, \"sum\": 2601.7019748687744, \"min\": 2601.7019748687744}}, \"EndTime\": 1603468394.204569, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468391.602785}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=249.824070112 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] #quality_metric: host=algo-1, epoch=2, train loss <loss>=4.02895337885\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] Saved checkpoint to \"/opt/ml/model/state_9eaee32f-825e-4200-bdcc-a8a7cffa3b38-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 97.4881649017334, \"sum\": 97.4881649017334, \"min\": 97.4881649017334}}, \"EndTime\": 1603468394.302721, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468394.204654}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] Epoch[3] Batch[0] avg_epoch_loss=4.107186\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:14 INFO 139658784810816] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=4.10718584061\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:15 INFO 139658784810816] Epoch[3] Batch[5] avg_epoch_loss=4.028528\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:15 INFO 139658784810816] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=4.02852757772\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:15 INFO 139658784810816] Epoch[3] Batch [5]#011Speed: 308.61 samples/sec#011loss=4.028528\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:16 INFO 139658784810816] Epoch[3] Batch[10] avg_epoch_loss=4.307352\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:16 INFO 139658784810816] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=4.64194021225\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:16 INFO 139658784810816] Epoch[3] Batch [10]#011Speed: 311.46 samples/sec#011loss=4.641940\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:16 INFO 139658784810816] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2554.011106491089, \"sum\": 2554.011106491089, \"min\": 2554.011106491089}}, \"EndTime\": 1603468396.856887, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468394.302791}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:16 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=252.139466181 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:16 INFO 139658784810816] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:16 INFO 139658784810816] #quality_metric: host=algo-1, epoch=3, train loss <loss>=4.30735150251\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:16 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:17 INFO 139658784810816] Epoch[4] Batch[0] avg_epoch_loss=4.194316\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:17 INFO 139658784810816] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=4.19431591034\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:18 INFO 139658784810816] Epoch[4] Batch[5] avg_epoch_loss=4.023479\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:18 INFO 139658784810816] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=4.02347934246\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:18 INFO 139658784810816] Epoch[4] Batch [5]#011Speed: 306.71 samples/sec#011loss=4.023479\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:19 INFO 139658784810816] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2417.6578521728516, \"sum\": 2417.6578521728516, \"min\": 2417.6578521728516}}, \"EndTime\": 1603468399.27509, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468396.856972}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:19 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=261.395409251 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:19 INFO 139658784810816] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:19 INFO 139658784810816] #quality_metric: host=algo-1, epoch=4, train loss <loss>=4.0011496067\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:19 INFO 139658784810816] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:19 INFO 139658784810816] Saved checkpoint to \"/opt/ml/model/state_16808c88-6687-4088-a3a5-c4ff34b9063d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 99.9901294708252, \"sum\": 99.9901294708252, \"min\": 99.9901294708252}}, \"EndTime\": 1603468399.375708, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468399.275182}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:19 INFO 139658784810816] Epoch[5] Batch[0] avg_epoch_loss=4.073389\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:19 INFO 139658784810816] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=4.07338857651\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:20 INFO 139658784810816] Epoch[5] Batch[5] avg_epoch_loss=4.036764\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:20 INFO 139658784810816] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=4.03676374753\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:20 INFO 139658784810816] Epoch[5] Batch [5]#011Speed: 308.03 samples/sec#011loss=4.036764\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:21 INFO 139658784810816] Epoch[5] Batch[10] avg_epoch_loss=4.028564\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:21 INFO 139658784810816] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=4.01872348785\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:21 INFO 139658784810816] Epoch[5] Batch [10]#011Speed: 312.48 samples/sec#011loss=4.018723\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:21 INFO 139658784810816] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2572.970151901245, \"sum\": 2572.970151901245, \"min\": 2572.970151901245}}, \"EndTime\": 1603468401.948835, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468399.375782}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:21 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=265.049955047 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:21 INFO 139658784810816] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:21 INFO 139658784810816] #quality_metric: host=algo-1, epoch=5, train loss <loss>=4.0285636295\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:21 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:22 INFO 139658784810816] Epoch[6] Batch[0] avg_epoch_loss=3.909626\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:22 INFO 139658784810816] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.90962648392\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:23 INFO 139658784810816] Epoch[6] Batch[5] avg_epoch_loss=4.095896\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:23 INFO 139658784810816] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=4.09589572748\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:23 INFO 139658784810816] Epoch[6] Batch [5]#011Speed: 308.15 samples/sec#011loss=4.095896\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:24 INFO 139658784810816] Epoch[6] Batch[10] avg_epoch_loss=4.293672\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:24 INFO 139658784810816] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=4.53100280762\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:24 INFO 139658784810816] Epoch[6] Batch [10]#011Speed: 309.71 samples/sec#011loss=4.531003\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:24 INFO 139658784810816] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2618.3769702911377, \"sum\": 2618.3769702911377, \"min\": 2618.3769702911377}}, \"EndTime\": 1603468404.567744, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468401.94892}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:24 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=248.997042104 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:24 INFO 139658784810816] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:24 INFO 139658784810816] #quality_metric: host=algo-1, epoch=6, train loss <loss>=4.29367167299\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:24 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:25 INFO 139658784810816] Epoch[7] Batch[0] avg_epoch_loss=4.100671\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:25 INFO 139658784810816] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=4.10067081451\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:26 INFO 139658784810816] Epoch[7] Batch[5] avg_epoch_loss=4.183965\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:26 INFO 139658784810816] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=4.1839650472\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:26 INFO 139658784810816] Epoch[7] Batch [5]#011Speed: 313.17 samples/sec#011loss=4.183965\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] Epoch[7] Batch[10] avg_epoch_loss=3.938344\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=3.64359884262\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] Epoch[7] Batch [10]#011Speed: 311.88 samples/sec#011loss=3.643599\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2576.7199993133545, \"sum\": 2576.7199993133545, \"min\": 2576.7199993133545}}, \"EndTime\": 1603468407.145029, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468404.56783}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=259.233348914 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] #quality_metric: host=algo-1, epoch=7, train loss <loss>=3.93834404512\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] Saved checkpoint to \"/opt/ml/model/state_4f939ff1-3622-46d1-9d02-ac3f88b209ff-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 67.15202331542969, \"sum\": 67.15202331542969, \"min\": 67.15202331542969}}, \"EndTime\": 1603468407.212812, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468407.1451}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] Epoch[8] Batch[0] avg_epoch_loss=4.038198\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:27 INFO 139658784810816] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=4.03819847107\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:28 INFO 139658784810816] Epoch[8] Batch[5] avg_epoch_loss=4.150140\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:28 INFO 139658784810816] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=4.15014000734\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:28 INFO 139658784810816] Epoch[8] Batch [5]#011Speed: 303.59 samples/sec#011loss=4.150140\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:29 INFO 139658784810816] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2389.6071910858154, \"sum\": 2389.6071910858154, \"min\": 2389.6071910858154}}, \"EndTime\": 1603468409.602564, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468407.212879}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:29 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=263.208209584 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:29 INFO 139658784810816] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:29 INFO 139658784810816] #quality_metric: host=algo-1, epoch=8, train loss <loss>=4.07770316601\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:29 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:30 INFO 139658784810816] Epoch[9] Batch[0] avg_epoch_loss=3.678034\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:30 INFO 139658784810816] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=3.67803382874\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:31 INFO 139658784810816] Epoch[9] Batch[5] avg_epoch_loss=3.940257\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:31 INFO 139658784810816] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.94025719166\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:31 INFO 139658784810816] Epoch[9] Batch [5]#011Speed: 309.25 samples/sec#011loss=3.940257\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:31 INFO 139658784810816] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2372.3528385162354, \"sum\": 2372.3528385162354, \"min\": 2372.3528385162354}}, \"EndTime\": 1603468411.975515, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468409.602655}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:31 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=252.055737799 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:31 INFO 139658784810816] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:31 INFO 139658784810816] #quality_metric: host=algo-1, epoch=9, train loss <loss>=4.02504551411\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:31 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:32 INFO 139658784810816] Epoch[10] Batch[0] avg_epoch_loss=3.871457\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:32 INFO 139658784810816] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=3.87145662308\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:33 INFO 139658784810816] Epoch[10] Batch[5] avg_epoch_loss=4.160566\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:33 INFO 139658784810816] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=4.16056601206\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:33 INFO 139658784810816] Epoch[10] Batch [5]#011Speed: 313.05 samples/sec#011loss=4.160566\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:34 INFO 139658784810816] Epoch[10] Batch[10] avg_epoch_loss=4.066036\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:34 INFO 139658784810816] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=3.9525990963\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:34 INFO 139658784810816] Epoch[10] Batch [10]#011Speed: 312.68 samples/sec#011loss=3.952599\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:34 INFO 139658784810816] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2613.297939300537, \"sum\": 2613.297939300537, \"min\": 2613.297939300537}}, \"EndTime\": 1603468414.589458, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468411.975607}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:34 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=252.541976208 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:34 INFO 139658784810816] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:34 INFO 139658784810816] #quality_metric: host=algo-1, epoch=10, train loss <loss>=4.06603559581\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:34 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:35 INFO 139658784810816] Epoch[11] Batch[0] avg_epoch_loss=3.684454\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:35 INFO 139658784810816] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=3.68445444107\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:36 INFO 139658784810816] Epoch[11] Batch[5] avg_epoch_loss=4.058973\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:36 INFO 139658784810816] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=4.05897327264\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:36 INFO 139658784810816] Epoch[11] Batch [5]#011Speed: 303.55 samples/sec#011loss=4.058973\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:36 INFO 139658784810816] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2385.6520652770996, \"sum\": 2385.6520652770996, \"min\": 2385.6520652770996}}, \"EndTime\": 1603468416.97565, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468414.589544}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:36 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=259.45306514 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:36 INFO 139658784810816] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:36 INFO 139658784810816] #quality_metric: host=algo-1, epoch=11, train loss <loss>=4.06455578804\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:36 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:37 INFO 139658784810816] Epoch[12] Batch[0] avg_epoch_loss=4.118507\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:37 INFO 139658784810816] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=4.11850738525\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:38 INFO 139658784810816] Epoch[12] Batch[5] avg_epoch_loss=4.138254\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:38 INFO 139658784810816] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=4.13825408618\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:38 INFO 139658784810816] Epoch[12] Batch [5]#011Speed: 313.67 samples/sec#011loss=4.138254\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:39 INFO 139658784810816] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2419.282913208008, \"sum\": 2419.282913208008, \"min\": 2419.282913208008}}, \"EndTime\": 1603468419.395587, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468416.975743}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:39 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=262.872815915 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:39 INFO 139658784810816] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:39 INFO 139658784810816] #quality_metric: host=algo-1, epoch=12, train loss <loss>=4.03108522892\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:39 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:39 INFO 139658784810816] Epoch[13] Batch[0] avg_epoch_loss=3.648435\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:39 INFO 139658784810816] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=3.6484348774\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:40 INFO 139658784810816] Epoch[13] Batch[5] avg_epoch_loss=3.890913\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:40 INFO 139658784810816] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=3.89091340701\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:40 INFO 139658784810816] Epoch[13] Batch [5]#011Speed: 299.31 samples/sec#011loss=3.890913\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] Epoch[13] Batch[10] avg_epoch_loss=3.821057\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=3.7372294426\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] Epoch[13] Batch [10]#011Speed: 300.08 samples/sec#011loss=3.737229\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2663.594961166382, \"sum\": 2663.594961166382, \"min\": 2663.594961166382}}, \"EndTime\": 1603468422.05981, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468419.395679}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=251.523963744 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] #quality_metric: host=algo-1, epoch=13, train loss <loss>=3.82105705955\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] Saved checkpoint to \"/opt/ml/model/state_ffca43c3-af21-4929-8002-bb01abe158af-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 78.37700843811035, \"sum\": 78.37700843811035, \"min\": 78.37700843811035}}, \"EndTime\": 1603468422.138837, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468422.059934}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] Epoch[14] Batch[0] avg_epoch_loss=3.813723\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:42 INFO 139658784810816] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=3.81372284889\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:43 INFO 139658784810816] Epoch[14] Batch[5] avg_epoch_loss=3.969003\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:43 INFO 139658784810816] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=3.96900328\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:43 INFO 139658784810816] Epoch[14] Batch [5]#011Speed: 315.54 samples/sec#011loss=3.969003\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:44 INFO 139658784810816] Epoch[14] Batch[10] avg_epoch_loss=3.806447\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:44 INFO 139658784810816] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=3.61137876511\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:44 INFO 139658784810816] Epoch[14] Batch [10]#011Speed: 312.25 samples/sec#011loss=3.611379\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:44 INFO 139658784810816] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2550.9119033813477, \"sum\": 2550.9119033813477, \"min\": 2550.9119033813477}}, \"EndTime\": 1603468424.689892, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468422.13891}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:44 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=254.797930661 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:44 INFO 139658784810816] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:44 INFO 139658784810816] #quality_metric: host=algo-1, epoch=14, train loss <loss>=3.80644668232\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:44 INFO 139658784810816] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:44 INFO 139658784810816] Saved checkpoint to \"/opt/ml/model/state_9d2e3bae-74a1-4090-8e46-9a559915e552-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 93.8720703125, \"sum\": 93.8720703125, \"min\": 93.8720703125}}, \"EndTime\": 1603468424.784366, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468424.689977}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:45 INFO 139658784810816] Epoch[15] Batch[0] avg_epoch_loss=3.867443\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:45 INFO 139658784810816] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=3.86744260788\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:46 INFO 139658784810816] Epoch[15] Batch[5] avg_epoch_loss=3.867557\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:46 INFO 139658784810816] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=3.86755720774\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:46 INFO 139658784810816] Epoch[15] Batch [5]#011Speed: 308.84 samples/sec#011loss=3.867557\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:47 INFO 139658784810816] Epoch[15] Batch[10] avg_epoch_loss=3.930088\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:47 INFO 139658784810816] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=4.00512537956\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:47 INFO 139658784810816] Epoch[15] Batch [10]#011Speed: 310.58 samples/sec#011loss=4.005125\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:47 INFO 139658784810816] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2570.3418254852295, \"sum\": 2570.3418254852295, \"min\": 2570.3418254852295}}, \"EndTime\": 1603468427.354873, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468424.784455}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:47 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=256.372958152 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:47 INFO 139658784810816] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:47 INFO 139658784810816] #quality_metric: host=algo-1, epoch=15, train loss <loss>=3.93008819493\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:47 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:47 INFO 139658784810816] Epoch[16] Batch[0] avg_epoch_loss=3.732402\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:47 INFO 139658784810816] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=3.73240232468\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:48 INFO 139658784810816] Epoch[16] Batch[5] avg_epoch_loss=3.920584\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:48 INFO 139658784810816] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=3.92058372498\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:48 INFO 139658784810816] Epoch[16] Batch [5]#011Speed: 313.58 samples/sec#011loss=3.920584\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:49 INFO 139658784810816] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2364.873170852661, \"sum\": 2364.873170852661, \"min\": 2364.873170852661}}, \"EndTime\": 1603468429.720277, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468427.35496}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:49 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=249.895978633 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:49 INFO 139658784810816] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:49 INFO 139658784810816] #quality_metric: host=algo-1, epoch=16, train loss <loss>=3.6780649066\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:49 INFO 139658784810816] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:49 INFO 139658784810816] Saved checkpoint to \"/opt/ml/model/state_ac3a9225-8c2a-4b13-8958-2556adc3508a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 67.83699989318848, \"sum\": 67.83699989318848, \"min\": 67.83699989318848}}, \"EndTime\": 1603468429.788787, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468429.720349}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:50 INFO 139658784810816] Epoch[17] Batch[0] avg_epoch_loss=4.143261\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:50 INFO 139658784810816] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=4.14326095581\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:51 INFO 139658784810816] Epoch[17] Batch[5] avg_epoch_loss=4.070295\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:51 INFO 139658784810816] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=4.07029489676\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:51 INFO 139658784810816] Epoch[17] Batch [5]#011Speed: 312.78 samples/sec#011loss=4.070295\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:52 INFO 139658784810816] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2338.2060527801514, \"sum\": 2338.2060527801514, \"min\": 2338.2060527801514}}, \"EndTime\": 1603468432.127149, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468429.788858}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:52 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=269.421205755 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:52 INFO 139658784810816] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:52 INFO 139658784810816] #quality_metric: host=algo-1, epoch=17, train loss <loss>=3.94902505875\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:52 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:52 INFO 139658784810816] Epoch[18] Batch[0] avg_epoch_loss=3.750684\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:52 INFO 139658784810816] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=3.75068426132\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:53 INFO 139658784810816] Epoch[18] Batch[5] avg_epoch_loss=3.978395\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:53 INFO 139658784810816] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=3.97839518388\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:53 INFO 139658784810816] Epoch[18] Batch [5]#011Speed: 314.29 samples/sec#011loss=3.978395\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:54 INFO 139658784810816] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2343.111991882324, \"sum\": 2343.111991882324, \"min\": 2343.111991882324}}, \"EndTime\": 1603468434.470857, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468432.127241}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:54 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=267.150565907 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:54 INFO 139658784810816] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:54 INFO 139658784810816] #quality_metric: host=algo-1, epoch=18, train loss <loss>=4.08633921146\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:54 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:54 INFO 139658784810816] Epoch[19] Batch[0] avg_epoch_loss=4.315784\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:54 INFO 139658784810816] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=4.31578350067\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:56 INFO 139658784810816] Epoch[19] Batch[5] avg_epoch_loss=4.016721\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:56 INFO 139658784810816] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=4.01672116915\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:56 INFO 139658784810816] Epoch[19] Batch [5]#011Speed: 310.16 samples/sec#011loss=4.016721\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:56 INFO 139658784810816] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2372.8179931640625, \"sum\": 2372.8179931640625, \"min\": 2372.8179931640625}}, \"EndTime\": 1603468436.844265, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468434.470948}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:56 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=264.227531008 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:56 INFO 139658784810816] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:56 INFO 139658784810816] #quality_metric: host=algo-1, epoch=19, train loss <loss>=3.87325849533\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:56 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:57 INFO 139658784810816] Epoch[20] Batch[0] avg_epoch_loss=4.153238\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:57 INFO 139658784810816] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=4.15323829651\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:58 INFO 139658784810816] Epoch[20] Batch[5] avg_epoch_loss=4.127305\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:58 INFO 139658784810816] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=4.12730463346\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:58 INFO 139658784810816] Epoch[20] Batch [5]#011Speed: 311.22 samples/sec#011loss=4.127305\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:59 INFO 139658784810816] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2382.636070251465, \"sum\": 2382.636070251465, \"min\": 2382.636070251465}}, \"EndTime\": 1603468439.227494, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468436.844358}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:59 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=266.076310953 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:59 INFO 139658784810816] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:59 INFO 139658784810816] #quality_metric: host=algo-1, epoch=20, train loss <loss>=4.03572797775\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:59 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:59 INFO 139658784810816] Epoch[21] Batch[0] avg_epoch_loss=3.964457\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:53:59 INFO 139658784810816] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=3.96445727348\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:00 INFO 139658784810816] Epoch[21] Batch[5] avg_epoch_loss=3.994401\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:00 INFO 139658784810816] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=3.99440133572\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:00 INFO 139658784810816] Epoch[21] Batch [5]#011Speed: 312.58 samples/sec#011loss=3.994401\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:01 INFO 139658784810816] Epoch[21] Batch[10] avg_epoch_loss=3.980533\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:01 INFO 139658784810816] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=3.96389017105\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:01 INFO 139658784810816] Epoch[21] Batch [10]#011Speed: 293.94 samples/sec#011loss=3.963890\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:01 INFO 139658784810816] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2660.2981090545654, \"sum\": 2660.2981090545654, \"min\": 2660.2981090545654}}, \"EndTime\": 1603468441.88841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468439.227587}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:01 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=249.208107081 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:01 INFO 139658784810816] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:01 INFO 139658784810816] #quality_metric: host=algo-1, epoch=21, train loss <loss>=3.9805326245\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:01 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:02 INFO 139658784810816] Epoch[22] Batch[0] avg_epoch_loss=4.355638\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:02 INFO 139658784810816] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=4.35563755035\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:03 INFO 139658784810816] Epoch[22] Batch[5] avg_epoch_loss=4.039628\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:03 INFO 139658784810816] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=4.0396275123\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:03 INFO 139658784810816] Epoch[22] Batch [5]#011Speed: 309.95 samples/sec#011loss=4.039628\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:04 INFO 139658784810816] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2405.374050140381, \"sum\": 2405.374050140381, \"min\": 2405.374050140381}}, \"EndTime\": 1603468444.294318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468441.888497}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:04 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=262.729919939 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:04 INFO 139658784810816] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:04 INFO 139658784810816] #quality_metric: host=algo-1, epoch=22, train loss <loss>=3.95127284527\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:04 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:04 INFO 139658784810816] Epoch[23] Batch[0] avg_epoch_loss=4.183780\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:04 INFO 139658784810816] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=4.18378019333\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:05 INFO 139658784810816] Epoch[23] Batch[5] avg_epoch_loss=4.009922\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:05 INFO 139658784810816] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=4.00992238522\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:05 INFO 139658784810816] Epoch[23] Batch [5]#011Speed: 308.19 samples/sec#011loss=4.009922\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:06 INFO 139658784810816] Epoch[23] Batch[10] avg_epoch_loss=3.941386\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:06 INFO 139658784810816] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=3.8591422081\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:06 INFO 139658784810816] Epoch[23] Batch [10]#011Speed: 309.09 samples/sec#011loss=3.859142\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:06 INFO 139658784810816] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2567.1839714050293, \"sum\": 2567.1839714050293, \"min\": 2567.1839714050293}}, \"EndTime\": 1603468446.862141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468444.294411}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:06 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=264.088831171 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:06 INFO 139658784810816] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:06 INFO 139658784810816] #quality_metric: host=algo-1, epoch=23, train loss <loss>=3.94138594107\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:06 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:07 INFO 139658784810816] Epoch[24] Batch[0] avg_epoch_loss=4.060998\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:07 INFO 139658784810816] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=4.06099796295\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:08 INFO 139658784810816] Epoch[24] Batch[5] avg_epoch_loss=3.991418\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:08 INFO 139658784810816] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=3.99141792456\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:08 INFO 139658784810816] Epoch[24] Batch [5]#011Speed: 308.77 samples/sec#011loss=3.991418\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:09 INFO 139658784810816] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2367.033004760742, \"sum\": 2367.033004760742, \"min\": 2367.033004760742}}, \"EndTime\": 1603468449.229751, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468446.86223}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:09 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=269.097471265 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:09 INFO 139658784810816] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:09 INFO 139658784810816] #quality_metric: host=algo-1, epoch=24, train loss <loss>=4.02287144661\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:09 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:09 INFO 139658784810816] Epoch[25] Batch[0] avg_epoch_loss=3.583193\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:09 INFO 139658784810816] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=3.58319330215\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:10 INFO 139658784810816] Epoch[25] Batch[5] avg_epoch_loss=3.786350\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:10 INFO 139658784810816] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=3.78635032972\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:10 INFO 139658784810816] Epoch[25] Batch [5]#011Speed: 303.96 samples/sec#011loss=3.786350\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:11 INFO 139658784810816] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2357.9959869384766, \"sum\": 2357.9959869384766, \"min\": 2357.9959869384766}}, \"EndTime\": 1603468451.588394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468449.229842}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:11 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=251.894079012 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:11 INFO 139658784810816] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:11 INFO 139658784810816] #quality_metric: host=algo-1, epoch=25, train loss <loss>=3.71215639114\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:11 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:12 INFO 139658784810816] Epoch[26] Batch[0] avg_epoch_loss=4.606931\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:12 INFO 139658784810816] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=4.60693073273\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:13 INFO 139658784810816] Epoch[26] Batch[5] avg_epoch_loss=3.959344\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:13 INFO 139658784810816] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=3.95934394995\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:13 INFO 139658784810816] Epoch[26] Batch [5]#011Speed: 312.66 samples/sec#011loss=3.959344\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:14 INFO 139658784810816] Epoch[26] Batch[10] avg_epoch_loss=4.311429\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:14 INFO 139658784810816] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=4.73393201828\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:14 INFO 139658784810816] Epoch[26] Batch [10]#011Speed: 309.63 samples/sec#011loss=4.733932\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:14 INFO 139658784810816] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2559.670925140381, \"sum\": 2559.670925140381, \"min\": 2559.670925140381}}, \"EndTime\": 1603468454.148643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468451.588486}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:14 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=255.879469943 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:14 INFO 139658784810816] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:14 INFO 139658784810816] #quality_metric: host=algo-1, epoch=26, train loss <loss>=4.31142943556\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:14 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:14 INFO 139658784810816] Epoch[27] Batch[0] avg_epoch_loss=3.817977\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:14 INFO 139658784810816] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=3.81797671318\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:15 INFO 139658784810816] Epoch[27] Batch[5] avg_epoch_loss=4.064304\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:15 INFO 139658784810816] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=4.06430383523\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:15 INFO 139658784810816] Epoch[27] Batch [5]#011Speed: 311.47 samples/sec#011loss=4.064304\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:16 INFO 139658784810816] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2374.0170001983643, \"sum\": 2374.0170001983643, \"min\": 2374.0170001983643}}, \"EndTime\": 1603468456.523218, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468454.148729}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:16 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=265.357457753 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:16 INFO 139658784810816] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:16 INFO 139658784810816] #quality_metric: host=algo-1, epoch=27, train loss <loss>=4.06865134239\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:16 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:17 INFO 139658784810816] Epoch[28] Batch[0] avg_epoch_loss=3.794808\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:17 INFO 139658784810816] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=3.79480814934\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:18 INFO 139658784810816] Epoch[28] Batch[5] avg_epoch_loss=3.986014\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:18 INFO 139658784810816] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=3.98601400852\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:18 INFO 139658784810816] Epoch[28] Batch [5]#011Speed: 304.47 samples/sec#011loss=3.986014\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:18 INFO 139658784810816] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2369.9419498443604, \"sum\": 2369.9419498443604, \"min\": 2369.9419498443604}}, \"EndTime\": 1603468458.893758, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468456.523312}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:18 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=255.265851094 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:18 INFO 139658784810816] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:18 INFO 139658784810816] #quality_metric: host=algo-1, epoch=28, train loss <loss>=3.74797427654\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:18 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:19 INFO 139658784810816] Epoch[29] Batch[0] avg_epoch_loss=3.949348\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:19 INFO 139658784810816] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=3.94934844971\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:20 INFO 139658784810816] Epoch[29] Batch[5] avg_epoch_loss=3.739155\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:20 INFO 139658784810816] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=3.73915469646\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:20 INFO 139658784810816] Epoch[29] Batch [5]#011Speed: 312.23 samples/sec#011loss=3.739155\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:21 INFO 139658784810816] Epoch[29] Batch[10] avg_epoch_loss=3.778136\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:21 INFO 139658784810816] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=3.82491459846\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:21 INFO 139658784810816] Epoch[29] Batch [10]#011Speed: 313.77 samples/sec#011loss=3.824915\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:21 INFO 139658784810816] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2554.558038711548, \"sum\": 2554.558038711548, \"min\": 2554.558038711548}}, \"EndTime\": 1603468461.448903, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468458.89385}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:21 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=265.786106355 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:21 INFO 139658784810816] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:21 INFO 139658784810816] #quality_metric: host=algo-1, epoch=29, train loss <loss>=3.7781364701\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:21 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:21 INFO 139658784810816] Epoch[30] Batch[0] avg_epoch_loss=3.784240\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:21 INFO 139658784810816] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=3.7842400074\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:23 INFO 139658784810816] Epoch[30] Batch[5] avg_epoch_loss=3.647752\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:23 INFO 139658784810816] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=3.64775236448\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:23 INFO 139658784810816] Epoch[30] Batch [5]#011Speed: 306.83 samples/sec#011loss=3.647752\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:24 INFO 139658784810816] Epoch[30] Batch[10] avg_epoch_loss=4.020995\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:24 INFO 139658784810816] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=4.46888628006\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:24 INFO 139658784810816] Epoch[30] Batch [10]#011Speed: 305.89 samples/sec#011loss=4.468886\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:24 INFO 139658784810816] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2605.026960372925, \"sum\": 2605.026960372925, \"min\": 2605.026960372925}}, \"EndTime\": 1603468464.054468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468461.448987}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:24 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=247.586036989 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:24 INFO 139658784810816] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:24 INFO 139658784810816] #quality_metric: host=algo-1, epoch=30, train loss <loss>=4.02099505338\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:24 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:24 INFO 139658784810816] Epoch[31] Batch[0] avg_epoch_loss=4.126017\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:24 INFO 139658784810816] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=4.12601661682\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:25 INFO 139658784810816] Epoch[31] Batch[5] avg_epoch_loss=3.758898\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:25 INFO 139658784810816] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=3.75889750322\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:25 INFO 139658784810816] Epoch[31] Batch [5]#011Speed: 315.08 samples/sec#011loss=3.758898\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:26 INFO 139658784810816] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2341.5839672088623, \"sum\": 2341.5839672088623, \"min\": 2341.5839672088623}}, \"EndTime\": 1603468466.396588, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468464.054554}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:26 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=264.335631704 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:26 INFO 139658784810816] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:26 INFO 139658784810816] #quality_metric: host=algo-1, epoch=31, train loss <loss>=3.74224071503\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:26 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:26 INFO 139658784810816] Epoch[32] Batch[0] avg_epoch_loss=3.505729\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:26 INFO 139658784810816] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=3.50572919846\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:27 INFO 139658784810816] Epoch[32] Batch[5] avg_epoch_loss=3.886226\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:27 INFO 139658784810816] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=3.88622574011\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:27 INFO 139658784810816] Epoch[32] Batch [5]#011Speed: 312.83 samples/sec#011loss=3.886226\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:28 INFO 139658784810816] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2348.315954208374, \"sum\": 2348.315954208374, \"min\": 2348.315954208374}}, \"EndTime\": 1603468468.745558, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468466.396677}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:28 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=259.319535478 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:28 INFO 139658784810816] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:28 INFO 139658784810816] #quality_metric: host=algo-1, epoch=32, train loss <loss>=3.7586442709\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:28 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:29 INFO 139658784810816] Epoch[33] Batch[0] avg_epoch_loss=4.278114\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:29 INFO 139658784810816] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=4.27811431885\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:30 INFO 139658784810816] Epoch[33] Batch[5] avg_epoch_loss=3.988288\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:30 INFO 139658784810816] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=3.98828832308\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:30 INFO 139658784810816] Epoch[33] Batch [5]#011Speed: 313.94 samples/sec#011loss=3.988288\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:31 INFO 139658784810816] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2352.545976638794, \"sum\": 2352.545976638794, \"min\": 2352.545976638794}}, \"EndTime\": 1603468471.098689, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468468.74565}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:31 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=271.604683968 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:31 INFO 139658784810816] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:31 INFO 139658784810816] #quality_metric: host=algo-1, epoch=33, train loss <loss>=3.97841601372\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:31 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:31 INFO 139658784810816] Epoch[34] Batch[0] avg_epoch_loss=3.854416\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:31 INFO 139658784810816] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=3.85441589355\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:32 INFO 139658784810816] Epoch[34] Batch[5] avg_epoch_loss=4.019201\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:32 INFO 139658784810816] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=4.01920131842\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:32 INFO 139658784810816] Epoch[34] Batch [5]#011Speed: 302.86 samples/sec#011loss=4.019201\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:33 INFO 139658784810816] Epoch[34] Batch[10] avg_epoch_loss=3.993499\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:33 INFO 139658784810816] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=3.96265621185\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:33 INFO 139658784810816] Epoch[34] Batch [10]#011Speed: 300.87 samples/sec#011loss=3.962656\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:33 INFO 139658784810816] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2623.0430603027344, \"sum\": 2623.0430603027344, \"min\": 2623.0430603027344}}, \"EndTime\": 1603468473.722348, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468471.09878}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:33 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=248.55403644 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:33 INFO 139658784810816] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:33 INFO 139658784810816] #quality_metric: host=algo-1, epoch=34, train loss <loss>=3.99349899725\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:33 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:34 INFO 139658784810816] Epoch[35] Batch[0] avg_epoch_loss=3.922920\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:34 INFO 139658784810816] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=3.92291998863\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:35 INFO 139658784810816] Epoch[35] Batch[5] avg_epoch_loss=3.936769\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:35 INFO 139658784810816] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=3.93676920732\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:35 INFO 139658784810816] Epoch[35] Batch [5]#011Speed: 312.72 samples/sec#011loss=3.936769\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:36 INFO 139658784810816] Epoch[35] Batch[10] avg_epoch_loss=3.972638\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:36 INFO 139658784810816] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=4.01568088531\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:36 INFO 139658784810816] Epoch[35] Batch [10]#011Speed: 311.43 samples/sec#011loss=4.015681\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:36 INFO 139658784810816] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2548.3200550079346, \"sum\": 2548.3200550079346, \"min\": 2548.3200550079346}}, \"EndTime\": 1603468476.271219, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468473.722432}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:36 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=258.982064432 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:36 INFO 139658784810816] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:36 INFO 139658784810816] #quality_metric: host=algo-1, epoch=35, train loss <loss>=3.97263815186\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:36 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:36 INFO 139658784810816] Epoch[36] Batch[0] avg_epoch_loss=4.016285\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:36 INFO 139658784810816] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=4.01628494263\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:37 INFO 139658784810816] Epoch[36] Batch[5] avg_epoch_loss=3.911894\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:37 INFO 139658784810816] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=3.91189432144\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:37 INFO 139658784810816] Epoch[36] Batch [5]#011Speed: 312.96 samples/sec#011loss=3.911894\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:38 INFO 139658784810816] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2345.0379371643066, \"sum\": 2345.0379371643066, \"min\": 2345.0379371643066}}, \"EndTime\": 1603468478.616804, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468476.2713}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:38 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=269.491535335 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:38 INFO 139658784810816] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:38 INFO 139658784810816] #quality_metric: host=algo-1, epoch=36, train loss <loss>=4.025025177\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:38 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:39 INFO 139658784810816] Epoch[37] Batch[0] avg_epoch_loss=4.032625\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:39 INFO 139658784810816] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=4.03262519836\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:40 INFO 139658784810816] Epoch[37] Batch[5] avg_epoch_loss=3.905334\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:40 INFO 139658784810816] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=3.90533379714\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:40 INFO 139658784810816] Epoch[37] Batch [5]#011Speed: 311.99 samples/sec#011loss=3.905334\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:41 INFO 139658784810816] Epoch[37] Batch[10] avg_epoch_loss=3.697582\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:41 INFO 139658784810816] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=3.44828004837\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:41 INFO 139658784810816] Epoch[37] Batch [10]#011Speed: 300.63 samples/sec#011loss=3.448280\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:41 INFO 139658784810816] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2581.5391540527344, \"sum\": 2581.5391540527344, \"min\": 2581.5391540527344}}, \"EndTime\": 1603468481.198979, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468478.61688}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:41 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=258.74774221 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:41 INFO 139658784810816] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:41 INFO 139658784810816] #quality_metric: host=algo-1, epoch=37, train loss <loss>=3.69758209315\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:41 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:41 INFO 139658784810816] Epoch[38] Batch[0] avg_epoch_loss=4.116207\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:41 INFO 139658784810816] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=4.1162071228\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:42 INFO 139658784810816] Epoch[38] Batch[5] avg_epoch_loss=3.977279\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:42 INFO 139658784810816] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=3.97727898757\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:42 INFO 139658784810816] Epoch[38] Batch [5]#011Speed: 315.82 samples/sec#011loss=3.977279\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:43 INFO 139658784810816] Epoch[38] Batch[10] avg_epoch_loss=3.845694\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:43 INFO 139658784810816] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=3.68779149055\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:43 INFO 139658784810816] Epoch[38] Batch [10]#011Speed: 307.76 samples/sec#011loss=3.687791\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:43 INFO 139658784810816] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2574.9359130859375, \"sum\": 2574.9359130859375, \"min\": 2574.9359130859375}}, \"EndTime\": 1603468483.774465, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468481.199062}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:43 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=259.411105941 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:43 INFO 139658784810816] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:43 INFO 139658784810816] #quality_metric: host=algo-1, epoch=38, train loss <loss>=3.84569376165\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:43 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:44 INFO 139658784810816] Epoch[39] Batch[0] avg_epoch_loss=3.523839\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:44 INFO 139658784810816] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=3.52383923531\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:45 INFO 139658784810816] Epoch[39] Batch[5] avg_epoch_loss=3.804296\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:45 INFO 139658784810816] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=3.80429601669\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:45 INFO 139658784810816] Epoch[39] Batch [5]#011Speed: 309.88 samples/sec#011loss=3.804296\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:46 INFO 139658784810816] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2346.877098083496, \"sum\": 2346.877098083496, \"min\": 2346.877098083496}}, \"EndTime\": 1603468486.121918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468483.774549}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:46 INFO 139658784810816] #throughput_metric: host=algo-1, train throughput=264.167142167 records/second\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:46 INFO 139658784810816] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:46 INFO 139658784810816] #quality_metric: host=algo-1, epoch=39, train loss <loss>=3.89728057384\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:46 INFO 139658784810816] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:46 INFO 139658784810816] Final loss: 3.6780649066 (occurred at epoch 16)\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:46 INFO 139658784810816] #quality_metric: host=algo-1, train final_loss <loss>=3.6780649066\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:46 INFO 139658784810816] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:46 WARNING 139658784810816] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:46 INFO 139658784810816] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 863.1000518798828, \"sum\": 863.1000518798828, \"min\": 863.1000518798828}}, \"EndTime\": 1603468486.986108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468486.121997}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:47 INFO 139658784810816] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 1124.0568161010742, \"sum\": 1124.0568161010742, \"min\": 1124.0568161010742}}, \"EndTime\": 1603468487.247024, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468486.986199}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:47 INFO 139658784810816] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:47 INFO 139658784810816] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 37.76693344116211, \"sum\": 37.76693344116211, \"min\": 37.76693344116211}}, \"EndTime\": 1603468487.284923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468487.247099}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:47 INFO 139658784810816] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:47 INFO 139658784810816] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.04291534423828125, \"sum\": 0.04291534423828125, \"min\": 0.04291534423828125}}, \"EndTime\": 1603468487.285767, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468487.284984}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 3345.172882080078, \"sum\": 3345.172882080078, \"min\": 3345.172882080078}}, \"EndTime\": 1603468490.630892, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468487.285813}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, RMSE): 278.753483834\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, mean_absolute_QuantileLoss): 150923.96327371214\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, mean_wQuantileLoss): 0.3499042078312245\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, wQuantileLoss[0.1]): 0.1938557353041472\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, wQuantileLoss[0.2]): 0.30229606052114305\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, wQuantileLoss[0.3]): 0.37877538935522914\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, wQuantileLoss[0.4]): 0.4279186127693791\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, wQuantileLoss[0.5]): 0.45205334374127903\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, wQuantileLoss[0.6]): 0.4462116897536887\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, wQuantileLoss[0.7]): 0.4065585067538209\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, wQuantileLoss[0.8]): 0.33911235631225023\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #test_score (algo-1, wQuantileLoss[0.9]): 0.20235617597008318\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.349904207831\u001b[0m\n",
      "\u001b[34m[10/23/2020 15:54:50 INFO 139658784810816] #quality_metric: host=algo-1, test RMSE <loss>=278.753483834\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 106407.25994110107, \"sum\": 106407.25994110107, \"min\": 106407.25994110107}, \"setuptime\": {\"count\": 1, \"max\": 9.379863739013672, \"sum\": 9.379863739013672, \"min\": 9.379863739013672}}, \"EndTime\": 1603468490.703295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1603468490.630966}\n",
      "\u001b[0m\n",
      "\n",
      "2020-10-23 15:55:00 Uploading - Uploading generated training model\n",
      "2020-10-23 15:55:00 Completed - Training job completed\n",
      "Training seconds: 171\n",
      "Billable seconds: 171\n",
      "CPU times: user 819 ms, sys: 43.2 ms, total: 862 ms\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_output_path),\n",
    "    \"test\": \"{}/test/\".format(s3_output_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you pass a test set in this example, accuracy metrics for the forecast are computed and logged (see bottom of the log).\n",
    "You can find the definition of these metrics from [our documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html). You can use these to optimize the parameters and tune your model or use SageMaker's [Automated Model Tuning service](https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/) to tune the model for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint and predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + pd.Timedelta(1, unit='D')\n",
    "\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        #prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        prediction_index = pd.date_range(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create and endpoint that can be queried using our custom DeepARPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "Using already existing model: deepar-electricity-demo-2020-10-23-15-49-45-917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `predictor` object to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "respDF=predictor.predict(ts=timeseries[4], quantiles=[0.90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing predicted cost for next 10 hours  at 0.9 quantile. Please note, the data is generated using Device simulators and not recorded by actual sensors. Predicted cost can sometime display negative numbers, which will not be the case in case of real life scenario. Power company will never credit money for low consumption!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-11-26 22:00:00</th>\n",
       "      <td>83.705292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27 00:00:00</th>\n",
       "      <td>69.530205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27 02:00:00</th>\n",
       "      <td>91.871063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27 04:00:00</th>\n",
       "      <td>95.999496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27 06:00:00</th>\n",
       "      <td>93.150986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27 08:00:00</th>\n",
       "      <td>107.803635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27 10:00:00</th>\n",
       "      <td>90.474136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27 12:00:00</th>\n",
       "      <td>65.174561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27 14:00:00</th>\n",
       "      <td>85.985062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27 16:00:00</th>\n",
       "      <td>49.314369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0.9\n",
       "2018-11-26 22:00:00   83.705292\n",
       "2018-11-27 00:00:00   69.530205\n",
       "2018-11-27 02:00:00   91.871063\n",
       "2018-11-27 04:00:00   95.999496\n",
       "2018-11-27 06:00:00   93.150986\n",
       "2018-11-27 08:00:00  107.803635\n",
       "2018-11-27 10:00:00   90.474136\n",
       "2018-11-27 12:00:00   65.174561\n",
       "2018-11-27 14:00:00   85.985062\n",
       "2018-11-27 16:00:00   49.314369"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: deepar-electricity-demo-2018-12-24-09-01-23-219\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
